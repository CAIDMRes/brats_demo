{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The import pipeline consists of a series of steps needed to convert and organize raw heterogenous data sources (DICOM objects, NIIGZ, MHA, PNG, JPG, etc) into a uniform, easy-to-access, NoSQL database-driven system. Once imported via this pipeline, any underlying data can be accessed using a standard modular interface. This facilitates ease of deep learning algorithm development as any imported dataset (or data subsets) can be easily used as input to a number of arbitrary algorithm architectures with minimal change in code.\n",
    "\n",
    "This tutorial and series of unit tests covers an overview of the data import pipeline including: \n",
    "\n",
    "1. Application context\n",
    "2. MongoDB document (JSON) schema \n",
    "3. Using `Importer()` class\n",
    "4. Using `Manager()` class \n",
    "\n",
    "The cnn package for implementing all functionality described here can be imported with a single module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application context\n",
    "\n",
    "The `app_context` is a python dictionary used to define the behavior of database interactions. When used during data import, it defines the target database name (and collection) within the MongoDB that data will be placed into (in addition to MongoDB network settings such as IP address/port if necessary). When used during data access, it defines the source database name (and collection) within the MongoDB from which data will be loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the application context\n",
    "\n",
    "There are several ways to define an application context. The simplest is to manually define the app_context dictionary. When manually defining an application context in this way, a minimum of two fields are required:\n",
    "\n",
    "* `name`: the name of the application context\n",
    "* `db`: the name of the Mongo database\n",
    "\n",
    "Note that the application context itself is a composite of configuration settings including the underlying Mongo `db` in addition to unique settings such as how the underlying data will be loaded, preprocessed or filtered. Thus any given Mongo `db` may be part of a number of application contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_app_context():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Define an app_context manually\n",
    "      \n",
    "    \"\"\"\n",
    "    app_context = {\n",
    "        'name': 'mnist_test',    # name of application context\n",
    "        'db': 'mnist'            # name of Mongo database\n",
    "    }\n",
    "    \n",
    "    return app_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As needed, many classes and methods in this package will fill in the app_context with other default values automatically. If for some reason you need to manually fill in the default values yourself, you can use the `cnn.db.init_app_context()` method as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_app_context():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Define an app_context manually\n",
    "      (2) Initialize the app_context with additional default values\n",
    "      \n",
    "    \"\"\"\n",
    "    app_context = define_app_context()\n",
    "    app_context = cnn.db.init_app_context(app_context)\n",
    "    \n",
    "    return app_context\n",
    "\n",
    "%time init_app_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving, loading and deleting application contexts\n",
    "\n",
    "As needed, `app_context` dictionaries can also be saved into a special Mongo database named `app`. The pertinent methods include:\n",
    "\n",
    "* `cnn.db.save_app_context(app_context)`\n",
    "* `cnn.db.remove_app_context(app_context`\n",
    "\n",
    "Note also that once an `app_context` has been saved, the `cnn.db.init_app_context()` method will first cross-reference any matching app_contexts in the database (based on provided `app_context['name']`) before using the generic default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_app_context():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Initialize an app_context \n",
    "      (2) Changing a single field\n",
    "      (3) Saving the modified app_context into the database\n",
    "      \n",
    "    \"\"\"\n",
    "    app_context = init_app_context()\n",
    "    app_context['dst_root'] = '/some/random/path'\n",
    "    cnn.db.save_app_context(app_context)\n",
    "    \n",
    "    return app_context\n",
    "\n",
    "%time save_app_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_app_context():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Load a saved app_context\n",
    "      \n",
    "    \"\"\"\n",
    "    app_context = cnn.db.init_app_context({'name': 'mnist_test'})\n",
    "    \n",
    "    return app_context\n",
    "\n",
    "%time load_app_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_app_context():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Delete a saved app_context\n",
    "      \n",
    "    \"\"\"\n",
    "    cnn.db.remove_app_context({'name': 'mnist_test'})\n",
    "    \n",
    "%time remove_app_context()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB\n",
    "\n",
    "## Document schema\n",
    "\n",
    "Each document in the database is a single dictionary (JSON object) that represents a single imaging study. This single document contains:\n",
    "\n",
    "1. All series obtained in the study.\n",
    "2. All labels for the study.\n",
    "3. Any additional metadata.\n",
    "\n",
    "General organization is as follows:\n",
    "\n",
    "```\n",
    "doc = {\n",
    "  'series': [list of series],\n",
    "  'labels': [list of labels],\n",
    "  'study': {dict of additional metadata ...}\n",
    "}\n",
    "```\n",
    "\n",
    "### Series-level information \n",
    "\n",
    "`doc['series']` contains a list dictionaries, each dictionary representing information from a single series. Thus, `doc['series'][0]` contains the first series, `doc['series'][1]` the second series, etc. Any given series dictionary contains the following structure:\n",
    "\n",
    "```\n",
    "doc['series'][0] = {\n",
    "  'seriesid': (str) id, \n",
    "  'description': (str) description of series,\n",
    "  'data': [list of data information]\n",
    "}\n",
    "```\n",
    "\n",
    "The `data` entry in the series dictionary contains a list of dictionaries, each dictionary representing specific information about the series. This information is stored as a list so that, if necessary, multiple transformations of the same underlying data can be stored into the document, for example if one wishes to store both the original and co-registered versions of the same series. In this case, the original data will could be stored in `doc['series'][0]['data'][0]` while the co-registered data volume could be stored in `doc['series'][0]['data'][1]`. Any given data dictionary contains the following structure:\n",
    "\n",
    "```\n",
    "doc['series'][0]['data'][0] = {\n",
    "  'file': (str) path to imported *.mvk file,\n",
    "  'hash': (str) unique hash for voxel data,\n",
    "  'dims': (list) Z x I x J describing voxel size,\n",
    "  'shape': (list) Z x I x J x N describing volume shape,\n",
    "  'slices': (int) # of slices,\n",
    "  'tags': (list) tags that describe data,\n",
    "  'type': (str) description of image space (default = 'orig'),\n",
    "  'name': (str) unique name describing data (optional)\n",
    "} \n",
    "```\n",
    "\n",
    "### Labels-level information \n",
    "\n",
    "`doc['labels']` contains a list of dictionaries, each dictionary represeting information from a single label. Thus, `doc['labels'][0]` contains the first label, `doc['labels'][1]` contains the second label, etc. Any given labels dictionary contains the following structure (same as series-data):\n",
    "\n",
    "```\n",
    "doc['labels'][0] = {\n",
    "  'file': (str) path to imported *.mzl file,\n",
    "  'hash': (str) unique hash for voxel data,\n",
    "  'dims': (list) Z x I x J describing voxel size,\n",
    "  'shape': (list) Z x I x J x N describing volume shape,\n",
    "  'slices': (int) # of slices,\n",
    "  'tags': (list) tags that describe data,\n",
    "  'type': (str) description of image space (default = 'orig'),\n",
    "  'name': (str) unique name describing data (optional),\n",
    "  'bounding_box': coordinates of bounding box,\n",
    "  'nnz': # of nonzero voxels for each label value,\n",
    "  'max_slice': slice location of maximum label 2D area \n",
    "}\n",
    "```\n",
    "\n",
    "### Meta-data \n",
    "\n",
    "`doc['study']` contains remaining metadata about the study. This dictionary can be modified to include more/less anonymized DICOM header data based on level of detail required for particular use case. Common fields include: \n",
    "\n",
    "```\n",
    "doc['study'] = {\n",
    "  'studyid': (str) identifier for study (commonly accession number),\n",
    "  'patientid': (str) patient medical record number,\n",
    "  'dscription': (str) description to study type,\n",
    "  'date': (str) date of exam,\n",
    "  'slices': (int) number of slices for the entire exam\n",
    "  'valid': (list) validation cross-fold(s)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importer() class\n",
    "\n",
    "The `Importer()` class is used to create objects for importing data into the database. There are generally two primary steps in importing data:\n",
    "\n",
    "1. Creating a Python list of documents identifying files to import (commonly saved as a pickle file)\n",
    "2. Feeding the documents list into the importer() class for data import\n",
    "\n",
    "## MNIST data\n",
    "\n",
    "For purposes of this tutorial and unit testing, we will download a custom copy of the MNIST dataset organized into separate `*.npy` files. The label for each example is encoded in the name `[x]-[id_number]` for the first number `[x]` represents the ground-truth label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, subprocess \n",
    "\n",
    "def download_mnist():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Check if MNIST data exists currently\n",
    "      (2) If not, download and unzip the files into data/\n",
    "      \n",
    "    \"\"\"\n",
    "    mnist = glob.glob('**/mnist.zip', recursive=True)\n",
    "    if mnist == []:\n",
    "        \n",
    "        # Clone and unzip data\n",
    "        commands = [\n",
    "            'git clone https://github.com/peterchang77/data',\n",
    "            'unzip -q data/raw/mnist.zip -d data/raw',\n",
    "        ]\n",
    "        \n",
    "        for c in commands:\n",
    "            subprocess.run(c.split(' '))\n",
    "    \n",
    "    else:\n",
    "        print('MNIST data already present')\n",
    "\n",
    "%time download_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating import documents\n",
    "\n",
    "As described in the MongoDB schema above, each study is organized as a single document (JSON object). To import data into the MongoDB, first create a list of documents following the predefined schema structure, such that each document in the list corresponds to a study that you wish to import. Note that importing a document containing a `doc['study']['studyid']` value that already exists in the database will simply **merge** the data (series, labels, meta-data) from the new document into the existing MongoDB document; any data that has already been imported will be ignored, while any data that is new will be appended.\n",
    "\n",
    "It is a good idea to save the import documents list as a separate `*.pkl` file. This will help you keep track of which studies have been imported or not. Importantly, it will also serve as a key between the filename to import and the studyid within the MongoDB. The naming convention for this file is `documents_[XXX].pkl` where `[XXX]` is some sort of identifier (keep in mind that a typical workflow will incorporate multiple serial data imports over time.\n",
    "\n",
    "During this process, you will be responsible for creating a studyid for each imported exam. A few popular choices include the accession number (if importing from DICOM files), or hashed versions of the accession number or full path to file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document structure\n",
    "\n",
    "The documents follow the same JSON structure described above, with the one exception that the 'file' value will be set to the file **to be imported** (not a pointer to the already imported file). At minimum, each document must contain a `studyid` value and `file` paths for either `series` and/or `labels` level information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, pickle\n",
    "\n",
    "def create_documents():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Create list of import documents\n",
    "      (2) Save document as pickle file\n",
    "      \n",
    "    Note we assume that MNIST data is in data/raw/mnist/*.npy as downloaded above\n",
    "      \n",
    "    \"\"\"\n",
    "    # Find studies \n",
    "    npys = glob.glob('data/raw/mnist/*.npy')\n",
    "    \n",
    "    # Append one study that doesn't exist to check error logging\n",
    "    npys.append(npys[-1].replace('.npy', '_noexist.npy'))\n",
    "    \n",
    "    # Here studyid will just be defined by file basename (without *.npy)\n",
    "    studyid = lambda x : os.path.basename(x.split('.')[0])\n",
    "    \n",
    "    # Create documents list\n",
    "    documents = []\n",
    "    for n, npy in enumerate(npys):\n",
    "        \n",
    "        documents.append({\n",
    "            'study': {'studyid': studyid(npy)},\n",
    "            'series': [{\n",
    "                'data': [{\n",
    "                    'file': npy,\n",
    "                    'type': 'orig',\n",
    "                    'tags': ['mnist-all']\n",
    "                    }]\n",
    "                }]})\n",
    "    \n",
    "    os.makedirs('pkls', exist_ok=True)\n",
    "    pickle.dump(documents, open('pkls/documents_all.pkl', 'wb'))\n",
    "    \n",
    "%time create_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing documents \n",
    "\n",
    "Once the documents list has been created, we can now individually import them into the database. Note that the following steps are performed (automatically) during import:\n",
    "\n",
    "* anonymization (pixel data as file pointers, meta-data as MongoDB documents)\n",
    "* custom file format conversion (images into `*.mvk` files, labels into `*.mzl` files)\n",
    "* reshaping all tensors to 4D (N x H x W x C) convention\n",
    "* splitting data into cross-validation cohorts\n",
    "* save app_context into database\n",
    "* saving documents with import errors into logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, cnn\n",
    "\n",
    "def import_data():\n",
    "    \"\"\"\n",
    "    Example showing how to:\n",
    "    \n",
    "      (1) Prepare an app_context for import\n",
    "      (2) Instantiating a new Importer() object\n",
    "      (3) Importing data\n",
    "      (4) Split data in validation folds\n",
    "      \n",
    "    \"\"\"\n",
    "    # Use method above to define a test MNIST app_context\n",
    "    app_context = define_app_context()\n",
    "    \n",
    "    # Importantly, add an import destination directory if not previously defined\n",
    "    app_context['dst_root'] = 'data/mvk-mzl'\n",
    "    \n",
    "    # Create Importer() object, initialized with your app_context\n",
    "    importer = cnn.db.Importer(app_context)\n",
    "    \n",
    "    # Add your documents list and override\n",
    "    documents = pickle.load(open('pkls/documents_all.pkl', 'rb'))\n",
    "    importer.documents = documents\n",
    "    \n",
    "    # Import\n",
    "    importer.run()\n",
    "    \n",
    "    # Validation folds\n",
    "    importer.validation_split(opts={'folds': 5})\n",
    "    \n",
    "%time import_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class UnitTests(unittest.TestCase):\n",
    "    \n",
    "    def define_app_context(self):\n",
    "        \n",
    "        define_app_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
