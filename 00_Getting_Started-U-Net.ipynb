{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this notebook we introduce basic concepts and functionality regarding use of the custom `cnn` library. Generally, this tutorial will cover just enough to begin using the `cnn` tools; for in-depth coverage of functionality please refer to the individual notebooks dedicated to each component of the `cnn` pipeline:\n",
    "\n",
    "1. Import Data\n",
    "2. Database Access and Manipulation\n",
    "3. Annotation Platform\n",
    "4. Algorithm Training\n",
    "5. Inference and Validation\n",
    "\n",
    "**Note:** The links embedded in this document are designed to jump directly the subsection within the above notebooks for further detail, however due to some quirks in Jupyter might require two clicks (one to open the notebook de novo, a second to jump to the notebook subsection) for full redirection.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The key design paradigm of the `cnn` Python library is an interconnected network of classes that have been prebuilt with a number of key functionality needed to train deep learning algorithms, particularly those dealing with medical imaging. To train a new algorithm, simply overload (e.g. \"adapt\") key methods and/or variables of several key classes. The vast majority of common architectures can be quickly adapated for a particular problem in less than 100 lines of code, however can easily be extended to large, complex architectures as needed.\n",
    "\n",
    "At minimum, the two key classes that need to be customized are the `Client` class (used for defining how to load data) and the `Model` class (used for defining the underlying CNN architecture model / graph). These (and many other default classes) are then combined into a final `Network` object to facilitate training as well as algorithn inference and validation:\n",
    "```\n",
    "net = cnn.Network()\n",
    "net.Client = Client\n",
    "net.Model = Model\n",
    "\n",
    "net.initialize(\n",
    "    app_context=app_context, \n",
    "    fold=0)\n",
    "    \n",
    "net.train()\n",
    "```\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Choosing GPU in Jupyter Notebooks\n",
    "\n",
    "If you are using a Jupyter notebook to interface with code development on a machine with multiple GPUs, you must first start by selecting the GPU that you wish to use for network training. Keep in mind that by default Tensorflow will *allocate* every free GPU available, despite the fact that it cannot use more than a single GPU unless your code has been carefully parallelized. To select just a single GPU, use the Terminal command `nvidia-smi` to list the current status of the cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  7 18:45:37 2018       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 390.59                 Driver Version: 390.59                    |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:3B:00.0  On |                  N/A |\n",
      "| 23%   19C    P8    16W / 250W |    357MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 23%   21C    P8    15W / 250W |  10962MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce GTX 108...  Off  | 00000000:AF:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8    15W / 250W |  10789MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce GTX 108...  Off  | 00000000:D8:00.0 Off |                  N/A |\n",
      "| 23%   22C    P8    15W / 250W |  10789MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see which GPUs are free, look for large allocations of memory in the second column (`___MiB / 11178MiB`). Any GPU with a large fraction memory already allocated is likely in use. Once we identify an open GPU, use the following lines of code to select the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/envs/cnn/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cnn/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/miniconda/envs/cnn/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2016332587123280581\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10560965837\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6316781733683673909\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']= 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'            # Modify as needed\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "These lines of code are used to import the necessary modules including the custom `cnn` library. Note here that the custom `montage` module is a pylab wrapper for quickly visualizing 3D grayscale images. Keep in mind that if your environment is not already preconfigured, you will need to set `$PYTHONPATH` with the necessary pointers using the `setenv.sh` script provided at the root of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to viewer URL: http://160.87.25.45:7000/login.html?user=caidm "
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://160.87.25.45:7000/login.html?user=caidm\">link</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cnn\n",
    "import numpy as np\n",
    "from montage import montage\n",
    "from show import show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client Class\n",
    "\n",
    "The `cnn.utils.Client` class is used to interact with underlying database images. Before instantiating this class, one needs to provide the generic `Client` class with a context (`app_context` dict) that defines the data (and/or label) sources. The key `app_context` dictionary entries to define include:\n",
    "\n",
    "* db: the name of MongoDB database containing data\n",
    "* ip_mongodb: the IP address of the MongoDB daemon (by default, set to `127.0.0.1`)\n",
    "* tags-series: list containing tags for series to load\n",
    "* tags-labels: list containing tags for labels to load (if empty, no label mask is loaded)\n",
    "\n",
    "Note that `tags-labels` refers to 2D or 3D volume mask data that need to be loaded from a separate file. Any single dimensional label data such as those for simple classification are stored directly in the MongoDB and are loaded automatically by the client (no special `tags-labels` need to be specified). For more information regarding `app_context` definition and maintenance see [01 - Import Data - Application Context](01_Import_Data.ipynb#Application-context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this example, we will be using the pre-imported BRaTS challenge database containing MRI exams from patients with brain tumors, loaded into the MongoDB under a database named `brats`. In this particular dataset, each patient was scanned with a total of four different modalities (series)---T2 (`brats-t2`), FLAIR (`brats-flair`), T1 precontrast (`brats-t1pre`) and T1 postcontrast (`brats-t1post`) exams. Note that each exam added in this list will simply be loaded and stacked along the channel (fourth) dimension, in the same order as this provided list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define application context to data and/or label sources\n",
    "app_context = {\n",
    "    'db': 'brats',\n",
    "    'tags-series': ['brats-t2', 'brats-flair'],\n",
    "    'tags-labels': ['tumor']\n",
    "}\n",
    "\n",
    "# Pass application context to Client() class to instantiate\n",
    "client = cnn.utils.Client(app_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "To use the newly instantiated `Client` class, the simplest approach is via the `client.load_next()` method. Without any parameters, this will choose a random study and load its entire contents. The return variable is a dictionary, `vol`, containing two key entries:\n",
    "\n",
    "* `vol['dat']`: 4D image volume (N x H x W x C)\n",
    "* `vol['lbl']`: 4D label volume (N x H x W x C)\n",
    "\n",
    "Here N = number of slices (N = 1 for 2D images) and C = number of different channels / modalities (C = 1 for single channel images e.g. plain film or CT). Note that if no `tags-labels` are provided then `vol['lbl']` will simply contain an empty list. For more information regarding various methods to load data using the `Client` class, please refer to [02 Database Access and Manipulation - Loading Data](02_Database_Access_and_Manipulation.ipynb#Loading-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 240, 240, 2)\n",
      "(155, 240, 240, 1)\n"
     ]
    }
   ],
   "source": [
    "def test_client_load():\n",
    "    \n",
    "    vol = client.load_next()\n",
    "    \n",
    "    return vol\n",
    "    \n",
    "vol = test_client_load()\n",
    "print(vol['dat'].shape)\n",
    "print(vol['lbl'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view these 3D volumes, consider using the `montage()` function. To promote flexibility, `montage()` accepts a wide range of inputs from 2D to 4D matrices and simply makes a montage (tiled collage) of the volume along an ambiguous dimension (*non* height or width dimension shape > 1). For a 4D input of size N x H x W x C, the ambiguous dimensions are either the first (N) or last (C) dimension. If both N and C are greater than 1, N is by default the ambiguous dimension and C is automatically set to the first index; the resulting output display is a total of N tiles (slices) of the first volume channel. The chosen channel can of course alternatively be set by manually indexing into the array as needed. By contrast, if N is set to 1 and C is greater than 1, then all channels for a given slice will be displayed instead.\n",
    "\n",
    "Some common example commands:\n",
    "\n",
    "```\n",
    "dat = vol['dat']\n",
    "montage(dat)                   # Shows all slices of the first channel\n",
    "montage(dat[::10])             # Shows every 10th slice of the first channel\n",
    "montage(dat[..., 1])           # Shows all slices of the second channel\n",
    "montage(dat[80])               # Shows all channels of the 80th slice\n",
    "```\n",
    "\n",
    "Feel free to try out these various combinations below (on `vol['dat']` and/or `vol['lbl']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infos dictionary\n",
    "\n",
    "As needed, the default behavior for loading images can be specified using the `infos` dictionary. The most common specifications will relate to image resizing and/or slicing, and these in turn will most commonly be defined using the `shape` and `tiles` entries. Addition information for full `infos` dictionary use can be found in [02 - Database Access and Manipulation - Infos Dictionary](02_Database_Access_and_Manipulation.ipynb#Infos-dictionary).\n",
    "\n",
    "### Volume resizing\n",
    "\n",
    "To resize the entire image volume along any dimension, simply set the `infos['shape']` list to the desired target size. Keep in mind the N x H x W convention such that by default any 2D image should be defined as `[1, H, W]` (two-entry lists will fail). All specifications in the `infos` dictionary are automatically propogated to any associated label masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 120, 120, 2)\n",
      "(64, 120, 120, 1)\n"
     ]
    }
   ],
   "source": [
    "def test_client_load():\n",
    "    \n",
    "    # Resize 155 x 240 x 240 to 64 x 120 x 120\n",
    "    infos = {\n",
    "        'shape': [64, 120, 120],\n",
    "        'tiles': [0, 0, 0]\n",
    "    }\n",
    "    \n",
    "    vol = client.load_next(infos=infos)\n",
    "    \n",
    "    return vol\n",
    "\n",
    "vol = test_client_load()\n",
    "print(vol['dat'].shape)\n",
    "print(vol['lbl'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume slicing\n",
    "\n",
    "To slice an image volume (load a portion of an image), simply set the desired shape and change the corresponding `tiles` to a non-zero value indicating the stride *distance* in millimeters (mm). For example a shape of `[5, 240, 240]` with a tiles of `[1, 0, 0]` indicates that random 5 contiguous slices with 1-mm spacing will be returned, whereas a tiles of `[3, 0, 0]` will increasing the spacing to 3-mm (every 3rd slice). If the original raw images imported into the database do not containg information about slice spacing (e.g. already anonymized and pre-coverted from original DICOM objects) then a default setting of 1-mm is asssumed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 240, 240, 2)\n",
      "(5, 240, 240, 1)\n",
      "Remote server (160.87.25.45:27787) recieved dat: size 288000\n",
      "Remote server (160.87.25.45:27787) recieved lbl: size 1\n",
      "Ready for viewing\n"
     ]
    }
   ],
   "source": [
    "def test_client_load():\n",
    "    \n",
    "    infos = {\n",
    "        'shape': [5, 240, 240],\n",
    "        'tiles': [1, 0, 0]\n",
    "    }\n",
    "\n",
    "    vol = client.load_next(infos=infos)\n",
    "    \n",
    "    return vol\n",
    "\n",
    "vol = test_client_load()\n",
    "print(vol['dat'].shape)\n",
    "print(vol['lbl'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRATS Dataset\n",
    "\n",
    "Now that we understand how to load images, let us look at the BRATS dataset more closely. In addition to the four differnt modalities described above, a 3D mask label is provided for each patient containing one of four different labels at each voxel (3D pixel) in the exam:\n",
    "\n",
    "* 1 = non-enhancing tumor\n",
    "* 2 = edema\n",
    "* 3 = necrosis\n",
    "* 4 = enhancing tumor\n",
    "\n",
    "If any given slice contains a mask label value > 0 (1 to 4) then that slice contains some type of tumor component. If the mask label consists entirely of zeros, then it is background (no tumor present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading Client\n",
    "\n",
    "Now that we are familiar with use of the generic `Client` class (useful for exploring data, etc) let us now go over the key modifications needed to customize the class for a specific experiment. To do so, we will at minimum need to overload the `init_client()` and `get()` methods of the template `Client` class.\n",
    "\n",
    "An example overloaded class definition is provided below. Continue reading for more information about considerations for overloading the client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client(cnn.utils.Client):\n",
    "    \n",
    "    def init_client(self):\n",
    "        \"\"\"\n",
    "        Method to set default experiment specific variables\n",
    "        \n",
    "        \"\"\"\n",
    "        self.infos = {\n",
    "            'shape': [1, 240, 240],\n",
    "            'tiles': [1, 0, 0],\n",
    "            'valid_expansion': True\n",
    "        }\n",
    "        \n",
    "        self.inputs = {\n",
    "            'dtypes': {},\n",
    "            'shapes': {},\n",
    "            'classes': {}\n",
    "        }\n",
    "        \n",
    "        self.inputs['dtypes']['dat'] = 'float32'\n",
    "        self.inputs['shapes']['dat'] = [1, 240, 240, 4]\n",
    "        \n",
    "        self.inputs['dtypes']['dsc-tumor'] = 'int32'\n",
    "        self.inputs['shapes']['dsc-tumor'] = [1, 240, 240, 1]\n",
    "        self.inputs['classes']['dsc-tumor'] = 5\n",
    "        \n",
    "        self.dist = {\n",
    "            0: 0.20, \n",
    "            1: 0.20, \n",
    "            2: 0.20, \n",
    "            3: 0.20, \n",
    "            4: 0.20}\n",
    "        \n",
    "        self.mode = 'mixed'\n",
    "        self.iids = {}\n",
    "        self.x = {}\n",
    "        \n",
    "    def get(self, mode=None, random=True):\n",
    "        \"\"\"\n",
    "        Method to load a single train/valid study\n",
    "        \n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        next_doc = self.next_doc(mode=mode, random=True)\n",
    "        vol = self.load(doc=next_doc['doc'], infos=next_doc['infos'])\n",
    "        \n",
    "        # Preprocessing\n",
    "        if vol['dat'].any():\n",
    "            vol['dat'] = (vol['dat'] - np.mean(vol['dat'])) / np.std(vol['dat'])\n",
    "        else:\n",
    "            vol['dat'][:] = -1\n",
    "\n",
    "        # Create nested vol dictionary\n",
    "        vol['lbl'] = {'dsc-tumor': vol['lbl'] + 1}\n",
    "        \n",
    "        return self.return_get(next_doc, vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading the self.init_client() method\n",
    "\n",
    "The `init_client()` method is called upon initialization of a new `Client()` object to set default experiment-specific values for CNN training. The two main dictionaries to define in this method are `self.infos` and `self.inputs`.\n",
    "\n",
    "### Setting self.infos dictionary\n",
    "\n",
    "The `infos` dictionary defines how underlying data will be loaded from memory. The same conventions described above apply here; however rather than defining the dictionary manually (repeatedly) one dictionary is saved as a variable within the class and reused for all subsequent calls. For more information see [02 - Database Access and Manipulation - Infos Dictionary](02_Database_Access_and_Manipulation.ipynb#Infos-dictionary) for more information.\n",
    "\n",
    "### Setting self.inputs dictionary\n",
    "\n",
    "The `self.inputs` dictionary is used to define key information about inputs into the model, including both input image data and labels. For input images, the corresponding `dtype` and `shapes` (input shape) must be defined. By convention, dictionary key used to define input data is set to `'dat'`.\n",
    "```\n",
    "self.inputs['dtypes']['dat'] = 'float32'\n",
    "self.inputs['shapes']['dat'] = [1, 240, 240, 1]\n",
    "```\n",
    "For input labels, the corresponding `dtypes`, `shapes` and `classes` (total number of classes for classification tasks; set to 0 for regression tasks) must be defined. Note that all labels are assumed to be 4D masks with single value labels represented by a matrix of shape (1, 1, 1, 1). \n",
    "```\n",
    "self.inputs['dtypes']['sce-tumor'] = 'int32'\n",
    "self.inputs['shapes']['sce-tumor'] = [1, 1, 1, 1]\n",
    "self.inputs['classes']['sce-tumor'] = 2\n",
    "```\n",
    "The dictionary key used to define a label must be carefully defined; the library will in fact use the specificiation here to automatically identify logit scores and apply the appropriate loss function without any other user input. To accomplish this, the algorithm assumes that the keys follow a naming convention split into two parts separated by a hypthen (`xxx-xxxx`). The first three letters before the hypthen indicate the type of loss function to apply to this label. The available loss functions include:\n",
    "\n",
    "* `sce`: sigmoid cross-entropy\n",
    "* `dsc`: soft Dice score\n",
    "* `l1d`: L1 distance\n",
    "* `l2d`: L2 distance\n",
    "* `sl1:` smooth L1 loss (Huber)\n",
    "\n",
    "The second half of the key after the hypthen can be any descriptive label as long as the keys are consistent. Note that the keys chosen here must match the keys used in the `self.get()` method below, and potentially in the `cnn.Model()` class below if customizations are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading the self.get() method\n",
    "\n",
    "The `self.get()` method is called during each training iteration to get data prepared for feeding into a CNN. As documented in [02 - Database Access and Manipulation - Loading Data](02_Database_Access_and_Manipulation.ipynb#Loading-data), the easiest way to accomplish this is to simply use the `self.next_doc()` method to pick a random MongoDB document matching the prespecified criteria in app_context, and then feeding the document into `self.load()`. Given that the `self.load()` function will only load 2D or 3D mask volume files into the default `vol` dictionary, any 1D conventional classification label if present should be extracted from the corresponding MongoDB document (`next_doc['doc']`) manually. \n",
    "\n",
    "During the `self.get()` method, any number of preprocessing steps may also be included. Note that at this point the tensors remain as Numpy (not Tensorflow) arrays, making a number of preprocessing pipelines easy to implement.\n",
    "\n",
    "There are two data structures that must be returned at the end of this call and passed into the `self.return_get()` method. The first is the `next_doc` dictionary containing the MongoDB document as well as some related metadata. This is generated automatically as part of the call to `self.next_doc()`. The second is a nested dictionary, vol:\n",
    "\n",
    "```\n",
    "vol = {\n",
    "    'dat': (NumPy array)\n",
    "    'msk': msk,\n",
    "    'lbl': lbl\n",
    "}\n",
    "```\n",
    "Note that while the `dat` entry is assumed to contain one input volume, the `lbl` (and `msk`) entries can potentially contain more than one label (and mask). Because of this, while `dat` simply contains a single Numpy array, both `msk` and `lbl` contain dictionaries with a number of potential masks and labels specified by a corresponding key (that matches the same key defined above in `init_inputs()`.\n",
    "\n",
    "Here, `vol['msk']` references a dictionary containing special a mask(s) equal in shape to the label. At all locations where mask is 0, the loss will be masked and not contribute to backpropogation calculations. By default, the mask will be set to 1 (True) for all pixels (or voxels). \n",
    "```\n",
    "msk = {\n",
    "    'lbl-key00': ...,\n",
    "    'lbl-key01': ..., etc\n",
    "}\n",
    "```\n",
    "Here, `vol['lbl']` references a dictionary containing the label. Note that by convention, any label with a value of 0 is ignored (reserved for missing data); thus the first class in your label output should be labeled 1, the second class 2, etc.\n",
    "```\n",
    "lbl = {\n",
    "    'lbl-key00': ...,\n",
    "    'lbl-key01': ..., etc\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Client\n",
    "\n",
    "In order to confirm that the Client class has been defined and overloaded properly, we can initialize a test client and load an arbitrary number of cases shown below. Note that instead of the default `client.load()` method (a generic method which requires a number of manually passed argmuments) we instead use the new overloaded `client.get()` method to load images in our preconfigured settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_client_get():\n",
    "\n",
    "    client = Client(app_context=app_context)\n",
    "    client.prepare(fold=-1)\n",
    "    dats = []\n",
    "    \n",
    "    N = 36\n",
    "    for i in range(N):\n",
    "        \n",
    "        print('Loading study %03i / %03i' % (i + 1, N), end='\\r')\n",
    "        output = client.get()\n",
    "        output = client.create_inputs_dict(output, mode='train')\n",
    "        dats.append(output['dat'])\n",
    "    \n",
    "    montage(np.concatenate(dats))\n",
    "\n",
    "test_client_get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overloading cnn.Model()\n",
    "\n",
    "The `cnn.Model()` class provides an object for easy creation and neural network model architectures. Defining several key method overloads will prepare this object with the necessary modifications to be used for CNN training. At minimum, the two key methods that need to be overloaded are:\n",
    "\n",
    "* `self.init_hyperparams_custom()`\n",
    "* custom network definition function \n",
    "\n",
    "An example overloaded class definition is provided below. Continue reading for more information about considerations for overloading the `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(cnn.SemanticModel):\n",
    "\n",
    "    def init_hyperparams_custom(self):\n",
    "\n",
    "        self.params['save_dir'] = './exps/unet/exp01' \n",
    "        self.params['batch_size'] = 16 \n",
    "        self.params['iterations'] = 200 \n",
    "\n",
    "        self.params['enet_fn'] = self.create_enet_vgg\n",
    "        self.params['train_ratio'] = {'enet': 1}\n",
    "        self.params['learning_rate'] = {'enet': 1e-3}\n",
    "\n",
    "        self.params['stats_matrix_shape'] = [155, None, None]\n",
    "        self.params['stats_mode'] = 'agg'\n",
    "        self.params['stats_top_model_source'] = {\n",
    "            'name': 'enet',\n",
    "            'node': 'errors',\n",
    "            'target': 'dsc-tumor',\n",
    "            'key': 2}\n",
    "\n",
    "    def create_enet_vgg(self, inputs):\n",
    "\n",
    "        nn_struct = {}\n",
    "\n",
    "        # ------------------------------------------------\n",
    "        # | FEATURE MAP DIMENSIONS|\n",
    "        # L0: 240\n",
    "        # L1: 120-120\n",
    "        # L2: 060-060 \n",
    "        # L3: 030-030 \n",
    "        # L4: 015-015\n",
    "\n",
    "        nn_struct['channels_out'] = [4,\n",
    "            16, 16,\n",
    "            32, 32,\n",
    "            48, 48,\n",
    "            64, 64]\n",
    "\n",
    "        nn_struct['filter_size'] = [[1, 3, 3],\n",
    "            [1, 3, 3], [1, 3, 3],\n",
    "            [1, 3, 3], [1, 3, 3],\n",
    "            [1, 3, 3], [1, 3, 3],\n",
    "            [1, 3, 3], [1, 3, 3]]\n",
    "        \n",
    "        nn_struct['stride'] = [1,\n",
    "            [1, 2, 2], 1,\n",
    "            [1, 2, 2], 1,\n",
    "            [1, 2, 2], 1,\n",
    "            [1, 2, 2], 1]\n",
    "        \n",
    "        nn_struct['decoder'] = True\n",
    "\n",
    "        self.builder.create(\n",
    "            name='E',\n",
    "            nn_struct=nn_struct,\n",
    "            input_layer=inputs[0])\n",
    "\n",
    "        self.create_logits(name='E')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading the self.init_hyperparams_custom() method\n",
    "\n",
    "To define model hyperparameters, overload the `init_hyperparams_custom()` method. Some of the common hyperparameters (with default values) are shown here:\n",
    "\n",
    "```\n",
    "self.params = {\n",
    "    'save_dir': None,               # directory for saving model and metadata\n",
    "    'iterations': 1e6,              # number of training iterations\n",
    "    'batch_size': 1,                # batch size\n",
    "    'learning_rate': None,          # learning rate; no default value (must be set)\n",
    "    'train_ratio': None,            # ratio at which to train different subnetworks; no default value \n",
    "    'optimizer': None,              # optimizer type; by default Adam will be used\n",
    "    'adam_b1': 0.5,                 # b1 for Adam optimizer\n",
    "    'adam_b2': 0.999,               # b2 for Adam optimizer\n",
    "    'l2_beta': 0,                   # lambda constant for L2 regularization\n",
    "}\n",
    "```\n",
    "\n",
    "One key concept to note here is the design choice of *subnetworks*. As needed, defining multiple individual subnetworks using specific conventions (e.g. `lnet`, `enet`, `gnet`, etc) will allow the `cnn` library to orchestrate larger, more complex architectures automatically, and to coordinate the training of each component at specific ratios (specified in the `train_ratio` dictionary entry) and individual learning rates. However the vast majority of standard single-pass feed-forward architectures (classification, U-net, etc) will simply be implemented as just a single *subnetwork*. For standard classification algorithms (VGG, ResNet, Inception, etc) use `lnet` and for fully-convolutional expanding-contracting architectures (U-net, etc) use `enet`. The training ratio for these simple single subnetwork architectures is just `{'lnet': 1}` or `{'enet': 1}` indicating that no special ratio is needed. The corresponding `{'lnet_fn': _}` or `{'enet_fn': _}` simply indicates the particular model architecture function, defined below in the same class, to be used (allows a number of different architecture permutations to be defined in a single template)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture\n",
    "\n",
    "Models are created using the built-in `self.builder` object. To use the `self.builder.create()` three different parameters must be defined. The most important is the `nn_struct` dictionary which defines the structure of the network. This structure is composed predominantly as a series of lists, with each entry in the list corresponding to a single layer in the neural network. For example the first three layers of a CNN may be defined as follows:\n",
    "```\n",
    "nn_struct = {\n",
    "    'channels_out': [16, 32, 64...],\n",
    "    'filter_size': [[1, 3, 3], [1, 3, 3], [1, 3, 3]...],\n",
    "    'stride': [1, 1, 1...]\n",
    " }\n",
    "```\n",
    "\n",
    "In this particular specification, we defined a total of 3 layers, each consisting of 1x3x3 (essentially 2D) convolutional filters with output feature maps 16, 32 and 64 and with a stride of 1. Note that the input channel sizes are calculated automatically. By default, each of these convolutions will be also followed by a batch normalization operation and a ReLU nonlinearity unless otherwise specified. Some of the most common layer specifications are shown here in the order of implementation within a single layer block:\n",
    "```\n",
    "nn_struct = {\n",
    "    'add_preblock': [...],        # name of layer to add before conv (residual connection); default is None\n",
    "    'filter_size': [...],         # filter sizes (specify 3D filters of size [Z, H, W]); no default\n",
    "    'resize': [...],              # perform nearest neighbor resize (specify feature map of size [Z, H, W]); default is None\n",
    "    'batch_norm': [...],          # True to include; default is True\n",
    "    'add_postblock': [...],       # name of layer to add after conv (residual connection); default is None\n",
    "    'relu': [...],                # use 1 for ReLU, values <1 for leaky ReLU; default is 1\n",
    "    'dropout': [...],             # [0, 1] for rate_to_keep; default is None\n",
    "    'padding': [...],             # 'SAME' or 'VALID'; default is 'SAME'\n",
    "}\n",
    "```\n",
    "Note that for each of these options, a value of `None` will ignore this specific layer component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Network\n",
    "\n",
    "To train a network, we use the `cnn.Network` class. While a number of custom modifications may be applied, the default `Network` class will often suffice for common CNN implementations. After initializing a new `Network` class, simply attach your custom class definitions to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_context = {\n",
    "    'db': 'brats',\n",
    "    'tags-series': ['brats-t2', 'brats-flair', 'brats-t1pre', 'brats-t1post'],\n",
    "    'tags-labels': ['tumor']\n",
    "}\n",
    "\n",
    "net = cnn.Network()\n",
    "net.Client = Client\n",
    "net.Model = Model\n",
    "net.Stats = cnn.SemanticStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to initialize (build) the network. In this same step, the library will also inspect your entire database for your requested input data and labels and prepare stratified sampling strategies as needed. The `initialize()` call requires the two required arguments are `app_context` (as defined above) and the fold you wish to set as the validation fold (usually start with 0 and cycle through all the other folds iteratively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.912 : Preparing client\n",
      "137.963 : Finding all documents matching tags (0000191)\n",
      "137.975 : Stratifying documents by label (1.0000)\n",
      "137.712 : Initializing graph\n",
      "137.712 : Creating E-net\n",
      "139.781 : Adding logits, losses and errors\n",
      "140.688 : Adding optimizers\n",
      "144.681 : Initializing stats\n",
      "144.784 : Initializing tf.Session\n"
     ]
    }
   ],
   "source": [
    "net.initialize(\n",
    "    app_context=app_context, \n",
    "    fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your graph has been built and compiled by the library. At this point the network graph structure itself is stored in the `model` attribute of `net`. Each  graph is stored as a separate dictionary entry into `model`. By default all of your created graphs will have two versions: the default `train` version (standard weights) and a special `valid` version whose weights are composed of the exponential moving average(s) of the `train` version of the model. This is a slight modification and technique that has been known to provide slightly improved results than the standard training weights. Additionally, the validation graph will have all special training mechanisms removed such as dropout, etc.\n",
    "\n",
    "To see all the sizes of intermediate activation layers, select `activations` attribute of the graph of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E/dsc-tumor/layer01-1/conv': <tf.Tensor 'E/dsc-tumor/layer01-1/conv:0' shape=(16, 1, 240, 240, 5) dtype=float32>,\n",
       " 'E/layer01-1/batch_norm': <tf.Tensor 'E/layer01-1_1/truediv:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer01-1/conv': <tf.Tensor 'E/layer01-1/conv:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer01-1/relu': <tf.Tensor 'E/layer01-1_2/relu:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer02-1/batch_norm': <tf.Tensor 'E/layer02-1_1/truediv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer02-1/conv': <tf.Tensor 'E/layer02-1/conv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer02-1/relu': <tf.Tensor 'E/layer02-1_2/relu:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer02-2/batch_norm': <tf.Tensor 'E/layer02-2_1/truediv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer02-2/conv': <tf.Tensor 'E/layer02-2/conv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer02-2/relu': <tf.Tensor 'E/layer02-2_2/relu:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer03-1/batch_norm': <tf.Tensor 'E/layer03-1_1/truediv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer03-1/conv': <tf.Tensor 'E/layer03-1/conv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer03-1/relu': <tf.Tensor 'E/layer03-1_2/relu:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer03-2/batch_norm': <tf.Tensor 'E/layer03-2_1/truediv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer03-2/conv': <tf.Tensor 'E/layer03-2/conv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer03-2/relu': <tf.Tensor 'E/layer03-2_2/relu:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer04-1/batch_norm': <tf.Tensor 'E/layer04-1_1/truediv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer04-1/conv': <tf.Tensor 'E/layer04-1/conv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer04-1/relu': <tf.Tensor 'E/layer04-1_2/relu:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer04-2/batch_norm': <tf.Tensor 'E/layer04-2_1/truediv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer04-2/conv': <tf.Tensor 'E/layer04-2/conv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer04-2/relu': <tf.Tensor 'E/layer04-2_2/relu:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer05-1/batch_norm': <tf.Tensor 'E/layer05-1_1/truediv:0' shape=(16, 1, 15, 15, 64) dtype=float32>,\n",
       " 'E/layer05-1/conv': <tf.Tensor 'E/layer05-1/conv:0' shape=(16, 1, 15, 15, 64) dtype=float32>,\n",
       " 'E/layer05-1/relu': <tf.Tensor 'E/layer05-1_2/relu:0' shape=(16, 1, 15, 15, 64) dtype=float32>,\n",
       " 'E/layer05-2/batch_norm': <tf.Tensor 'E/layer05-2_1/truediv:0' shape=(16, 1, 15, 15, 64) dtype=float32>,\n",
       " 'E/layer05-2/conv': <tf.Tensor 'E/layer05-2/conv:0' shape=(16, 1, 15, 15, 64) dtype=float32>,\n",
       " 'E/layer05-2/relu': <tf.Tensor 'E/layer05-2_2/relu:0' shape=(16, 1, 15, 15, 64) dtype=float32>,\n",
       " 'E/layer06-1/add_postblock': <tf.Tensor 'E/layer06-1_2/add_postblock:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer06-1/batch_norm': <tf.Tensor 'E/layer06-1_1/truediv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer06-1/convt': <tf.Tensor 'E/layer06-1/convt:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer06-1/relu': <tf.Tensor 'E/layer06-1_3/relu:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer06-2/batch_norm': <tf.Tensor 'E/layer06-2_1/truediv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer06-2/conv': <tf.Tensor 'E/layer06-2/conv:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer06-2/relu': <tf.Tensor 'E/layer06-2_2/relu:0' shape=(16, 1, 30, 30, 48) dtype=float32>,\n",
       " 'E/layer07-1/add_postblock': <tf.Tensor 'E/layer07-1_2/add_postblock:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer07-1/batch_norm': <tf.Tensor 'E/layer07-1_1/truediv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer07-1/convt': <tf.Tensor 'E/layer07-1/convt:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer07-1/relu': <tf.Tensor 'E/layer07-1_3/relu:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer07-2/batch_norm': <tf.Tensor 'E/layer07-2_1/truediv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer07-2/conv': <tf.Tensor 'E/layer07-2/conv:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer07-2/relu': <tf.Tensor 'E/layer07-2_2/relu:0' shape=(16, 1, 60, 60, 32) dtype=float32>,\n",
       " 'E/layer08-1/add_postblock': <tf.Tensor 'E/layer08-1_2/add_postblock:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer08-1/batch_norm': <tf.Tensor 'E/layer08-1_1/truediv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer08-1/convt': <tf.Tensor 'E/layer08-1/convt:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer08-1/relu': <tf.Tensor 'E/layer08-1_3/relu:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer08-2/batch_norm': <tf.Tensor 'E/layer08-2_1/truediv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer08-2/conv': <tf.Tensor 'E/layer08-2/conv:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer08-2/relu': <tf.Tensor 'E/layer08-2_2/relu:0' shape=(16, 1, 120, 120, 16) dtype=float32>,\n",
       " 'E/layer09-1/add_postblock': <tf.Tensor 'E/layer09-1_2/add_postblock:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer09-1/batch_norm': <tf.Tensor 'E/layer09-1_1/truediv:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer09-1/convt': <tf.Tensor 'E/layer09-1/convt:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer09-1/relu': <tf.Tensor 'E/layer09-1_3/relu:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer09-2/batch_norm': <tf.Tensor 'E/layer09-2_1/truediv:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer09-2/conv': <tf.Tensor 'E/layer09-2/conv:0' shape=(16, 1, 240, 240, 4) dtype=float32>,\n",
       " 'E/layer09-2/relu': <tf.Tensor 'E/layer09-2_2/relu:0' shape=(16, 1, 240, 240, 4) dtype=float32>}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.model.graphs['E-train'].activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'dsc-tumor': <tf.Tensor 'E/dsc-tumor/layer01-1/conv:0' shape=(16, 1, 240, 240, 5) dtype=float32>},\n",
       " 'valid': {'dsc-tumor': <tf.Tensor 'E/dsc-tumor/layer01-1_1/conv:0' shape=(?, ?, 240, 240, 5) dtype=float32>}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.model.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we are now ready to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.812 : Starting training\n",
      "\n",
      "          |----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "          | enet                                                                                                                                         |\n",
      "          |----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "          | train                                                                      | valid                                                           |\n",
      "          | dsc-tumo | l2norm   | tumor-0  | tumor-1  | tumor-2  | tumor-3  | tumor-4  | dsc-tumo | tumor-0  | tumor-1  | tumor-2  | tumor-3  | tumor-4  |\n",
      "          |----------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "0000198 > | -10.2031 | 0.000000 | 0.854924 | 0.010035 | 0.127888 | 0.016015 | 0.294385 | -8.46875 | 0.861221 | 0.015426 | 0.127553 | 0.015242 | 0.287088 | "
     ]
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default training paradigm implemented by the `cnn` library, training and validation sets are evaluated simultaneously for real-time monitoring of current training dynamics at any given time point. All individual components of the loss function are reported, as are all defined errror metrics (e.g. top-K for classification, Dice score for segmentation, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreaded Training\n",
    "\n",
    "A significant bottleneck to CNN training is loading data into memory (and subsequently GPU) for training. Given the single-threaded nature of Python, typically the data loading process for the next iteration does not begin to occur until the current training iteration has completed. To use a custom asynchronous load function to significantly increase training speed, pass three additional parameters to the `initialize()` call: threads, batch and capacity:\n",
    "\n",
    "* threads: number of separate independent threads to use (consider the # of total CPU threads available on your machine)\n",
    "* batch: total number of exams to be loaded by each thread at a time in a single batch\n",
    "* capacity: total number of studies to be pre-loaded in the queue\n",
    "\n",
    "Note that the number of threads x batch should a multiple of the training batch size (minimum 2 to 3 times greater) otherwise one iteration of the asynchronous processs will not load enough data for a single pass through the network. The following default parameters are reasonable for our starting batch size of 16 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_context = {\n",
    "    'db': 'brats',\n",
    "    'ip_mongodb': '160.87.25.45',\n",
    "    'tags-series': ['brats-t2', 'brats-flair', 'brats-t1pre', 'brats-t1post'],\n",
    "    'tags-labels': ['tumor']\n",
    "}\n",
    "\n",
    "net = cnn.Network()\n",
    "net.Client = Client\n",
    "net.Model = Model\n",
    "\n",
    "net.initialize(\n",
    "    app_context=app_context, \n",
    "    fold=0,\n",
    "    threads=2, \n",
    "    batch=32,\n",
    "    capacity=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training statistics\n",
    "\n",
    "All training statistics are stored in a Pickled dictionary file. This file can be easily loaded and viewed using the `cnn.Viewer` class. To do so, simply pass the training directory to the initial class instantiation. To view training dynamcis over time, simply pass the subnetwork name (`lnet`, `enet`, etc), node (either `errors` or `losses`), target / label. For losses, no other information is required because the error type is defined by the label itself (e.g. `sce-` for sigmoid cross entropy). For errors, an error key is needed given that a number of possible error metrics may exist for a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the current Python kernel is engaged in algorithm training, it may be worthwhile to load a second kernel (new Jupyter notebook, new Python shell, etc) to concurrently graph training loss / error curves over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd4FWX2wPHvSackhBJKSCD0KjUiKlZEEUVsIFhWXXfd1WVtu+ta1l11ddW1u1h/diyI2FBRLAg2REKVUENNCCUQUiEhyT2/P2aAS0y5hCSTcj7Pkyf3zrwzc2buvXPmfd8poqoYY4wxQV4HYIwxpm6whGCMMQawhGCMMcZlCcEYYwxgCcEYY4zLEoIxxhjAEkKjJiJzReR3XsdRm0TkbhG52+s4yiIir4rIVTW8jGQRObUml1HGMk8VkbRAYihdtozxNb6NGjNLCFUkIptE5AwPl/+qiNzn1fLLIiIJIqIiEuJ1LHXN0X5e1bVtVbWfqs49guXGVrSDro0YTO2xhOAREQn2OoaGTByN6vtdQ4l4DPB5DczXU2V9P6rynWloBz+N6gdTXURkKtAJ+FhE8kTkVnf4uyKyXUSyReRbEennN82rIvKsiMwSkXzgNBFpLSIfi0iOiCwUkftE5Hu/aXqLyJcikikia0Rkgjv8WuAy4FZ3+R8HGPcoEVntxjcFEL9x3UVknjtul4i84zeun18cO0TkjnIW8a37P8uN63i3ieYNv3kddqTrNlvdJyI/HlgXd7u86bddEvymP8Edlu3+P8Fv3FwRuV9EfgD2Al0D2S6ltlGZ29wd96qIPC0in4pIrogsEJFulU1b1c+rlLK27VUi8oOIPC4imcDdItJNROaIyG73c3xTRKL9YjxYs3U/m+ki8rq7PskiklhquWOAWSJym4jMKLWtnhSRp9zXV4vIKnc+G0TkDxVsY/8YmrjbdY+IrASOrcK28Z/3cPe7lCUiy8Svaaqs70c5w2JFZKb7OaaIyO/95nG3iMwQkTdEJAe4SkSGiUiS+33dISKPHc06eEpV7a8Kf8Am4IxSw34LRALhwBPAUr9xrwLZwIk4iTgCmOb+NQX6AqnA9275Zu77q4EQYAiwC+jnN7/7Si3/GeCZcuJtA+QAFwOhwM1AMfA7d/zbwJ1+sY1wh0cC24C/uMMjgePKWUYCoECI37C7gTfKKwPMBVKAbkALYCWwFjjDXe/XgVfcsq2APcAV7rhJ7vvWfvPaAvRzx4eWEePdwN3lxB/INs8Ehrnj3wSmVfXzKmP5rwJXHcG2vcr9DP/sLrMJ0B0YhfMdjMFJJE+U9b11t0UBzk4/GHgA+MmvbKi7DpFAZ5wdZpQ7Ltj9Xgx335/jfoYCnOKWHeKOOxVIKyeGB4Hv3M82HljhX/YIt1FHYLe7PkHudtgNxJT3/Shn2Dyc31IEMAjIAEb6bbMi4Hx3GU2A+cAV7vjmB7ZJffyzGkI1UtWXVTVXVQtxvjgDRaSFX5GPVPUHVfXhfKkuAv6lqntVdSXwml/Zc4FNqvqKqhar6mLgPZwdennLv15Vry9n9BhgparOUNUinIS13W98Ec6PPlZVC1T1QE3lXGC7qj7qDs9V1QUBbpJAvaKq61U1G/gMWK+qX6lqMfAuMNgtdw6wTlWnutvkbWA1MNZvXq+qarI7vugI4whkm7+vqj+7sb2Js8MIdNqakK6q/3OXuU9VU1T1S1UtVNUM4DGcHXR5vlfVWapaAkwFBvqNOxlY5n7mm4HFODtCgNOBvar6E4Cqfup+hqqq84AvgJMCiH8CcL+qZqpqKvDUkax8KZcDs9z18anql0ASznf/gLK+HweHAe2BEcDf3e/7UuBFnIOQA+ar6ofuMvbh/Ha6i0gbVc07sE3qI0sI1UREgkXkQRFZ71YlN7mj2vgVS/V7HYNzRJJazvjOwHFu1TdLRLJwmh3aVzHEWP/5q3M447+8W3GO7n52mw5+6w6PB9aXNUO36eLAX6cqxgWww+/1vjLeN/dbh82lpt2Mc2R4QCpVF8g290+ie/1iq+7PK1CHra+ItBWRaSKy1f0evsHh38HSSq9PhBxqFx8DzPIb/xZOrQzgUvf9geWeLSI/uc0sWe60FS33gMO+l/z68z0SnYHxpT6DEUAHvzJlfT/8h8UCmaqaWyqmir5j1wA9gdVuM+a5VV4DjzWoDpFaVvo2sZcC43CaOjbhNH/swa+dvtQ0GTjV/TicJhJwdr4HpALzVHVUgMuvzDb/+YuI+L9X1e3A791xI4CvRORbN45JlEFVm/u/F5HOZRTLx2kSO+BodpDpOD96f504vNPzaG7fW9k2P5ppj/a2wuVNX3r4A+6wAaq6W0TOB6ZUcZljgAv83r8LPCoice7w4wFEJBynNvQbnFpwkYh8yOHf/fIc+F4mu++P5sAiFZiqqr+voExZ29F/WDrQSkQi/ZJCJ2BrefNQ1XXAJHE6pC8EZohIa1XNP+I18JjVEKpuB4d3WkYChThtlk2B/1Q0sVtFfx+nI7CpiPTG+UEd8AnQU0SuEJFQ9+9YEelTzvIr8ynQT0QudI8Ab8Bv5ywi490fOjiJTIESN472InKTiISLSKSIHFfOMjIAX6m4lgIni0gnt/ns9iOIubRZONvkUhEJEZFLcPpePjmKefqrbJsfzbRH+nmVVta2LUskkIfT+dwR+FtVFiYiXYBwVV19YJjbBDUXeAXYqKqr3FFhOH0WGUCxiJwNnBngoqYDt4tIS/f79+eqxOt6AxgrIme5NfYIca5riKt0SpfbbPUj8IA7/QCcGsCb5U0jIpeLSIzbFJzlDi45ivXwjCWEqnsA+IdbNf0rTufnZpwjiZVAIO2Ik3FqEttx2m/fxkkquEcnZwITcY5atgMP4fzwAF4C+rrL/xBARJ4TkefKWpCq7gLG43Ti7QZ6AD/4FTkWWCAiecBM4EZV3ejGMQqnnX47sA44rZxl7AXuB35w4xrutuO+AywHFnEUO29V3Y3TVv8Xdx1uBc511+2oBbDNj2baX31eRxjbr7ZtOUXvwenQzsY5CHj/SJflOofDm4sOeAunFnywuchd9xtwdu57cGrLMwNczj04v5uNOP0OU6sY74Gd+TjgDpzklIqTEI90PzcJpxM/HfgAp5/vywrKjwaS3d/Ok8BEVS04wmXWCeI0JZu6QEQeAtqr6pVex9JQiXuVsqre7W0kvyYirwJzVfVVj0NBRGYBU1S1rKTgmbq0jRoiqyF4SJzz1geIYxhO1fQDr+MyBqdp6BuvgzC1yzqVvRWJ00wUC+wEHgU+8jSihm+u1wFU4EMOnZ3mKVX9r9cxlKPObKOGyJqMjDHGANZkZIwxxlWvmozatGmjCQkJXodhjDH1yqJFi3apakxl5epVQkhISCApKcnrMIwxpl4RkYCuALcmI2OMMYAlBGOMMS5LCMYYYwBLCMYYY1yWEIwxxgCWEIwxxrgsIRhjjAEsIRhjTJ2VW1DE5yu289/PV1deuBrUqwvTjDGmIfP5lJXbcpi3NoN5azNYvHkPxT4lMjyEa0Z0oXXzSh/NcVQsIRhjjIcy8/fz3TonAXy7dhe78goB6N8xij+c0pVTerZlcKdoQoNrvkHHEoIxxtSi4hIfy9KymLcmg3nrdrE8LQtVaNk0lJN7tOHMhGBOjM4ieu8G2PU5zE+Bz7bAH3+AoJpNCpYQjDGmhm3PLuBbtxnou3UZFBTso0vQDkbG5PD3XnvoHbKdlvs2I5vXwZrsQxOGRECrbtCmB+zPg4ioGo3TEoIxxlSzwuISFm3MJCl5NWnrlhOStZ6uks6loTu4N3Q7rdiO4HOefJ0NRMZCm+7Q/2Jn59+6h/O/RXyN1wr8WUIwxpijUbQPdq8nY1MyW1OWsW/bGprlbaQ/6Zwg+5wyoeALiUBad0faDPfb6XeH1t0hPNLbdXAFlBBEZDTwJBAMvKiqD5YaHw68DgwFdgOXqOomEWkNzACOBV5V1cllzHsm0FVV+x/VmhhjTE1RhdxtsGst7FoHu1MoyVjL/h1ricjfiqDEADHADmlDbmQC2e2HE97lGMLb94LWPQiK6lirR/tVUWlCEJFg4GlgFJAGLBSRmaq60q/YNcAeVe0uIhOBh4BLgALgLqC/+1d63hcCeUe9FsYYU922LYP5z0DGKti93mnDdxVIBCm+Dqz3dWKLDCekbU86dh/AgIGJdG7fhnYiHgZedYHUEIYBKaq6AUBEpgHjAP+EMA642309A5giIqKq+cD3ItK99ExFpDlwC3AtML3Ka2CMMdVp3x6Ycz8kvYQvPIrdLfqzNnI0P2a3ZMneGDb4OhAZ04lTerXllF4xnJXQiojQYK+jrhaBJISOQKrf+zTguPLKqGqxiGQDrYFdFcz338CjwN6KFi4i1+IkDTp16hRAuMYYUwU+Hyx7G778J7ovk++jx3HDjnPYk9WUyPAQTuzehrG9YjilZwyx0U28jrZGBJIQyqr7aBXKHCosMgjorqo3i0hCRQtX1ReAFwASExPLnacxxlTZtuXorL8iqQtICe/LDQW3sGl3NyYMj2fMMR1q7cIwrwWSENKAeL/3cUB6OWXSRCQEaAFkVjDP44GhIrLJjaGtiMxV1VMDjNsYY47evix839yP/PwiOdKc+4qu5ZugM7jyjK5ccXxnopuGeR1hrQokISwEeohIF2ArMBG4tFSZmcCVwHzgYmCOqpZ7NK+qzwLPArg1hE8sGRhjao0q+xe/hW/2Pwjdn8UbxSOZFnklk84awL+HxjWYPoEjVWlCcPsEJgOzcU47fVlVk0XkXiBJVWcCLwFTRSQFp2Yw8cD0bi0gCggTkfOBM0udoWSMMbUmd/NSct+/kdjspSz2dee1lv9k1Miz+Lh/B4KD6ufZQdVFKjiQr3MSExM1KSnJ6zCMMfXQjp072fzePxiy/V2yacb7rX5P3zHXcUL3GKSeniYaKBFZpKqJlZWzK5WNMQ1ayo4cFs58njPS/kciOfzQ8jzajP03v+/W2evQ6hxLCMaYBmnR5kw++uJrzkl9hElBq0lr1pedY6dxUp8TvA6tzrKEYIxpMHw+5evVO3n9m+Wcsu0l/hkym6KwSPJPf4y44VfX+VtHeM0SgjGm3ttf7OPDpVt5Yd56+u7+gsfD3qJ1SBbFg6+kyah/QdNWXodYL1hCMMbUW7kFRbz98xZe/n4TkbkpPNpsKgPDfsHXYTBy7qOEdhzqdYj1iiUEY0y9szO3gFd+2MQbP23GV5DLg60/45yID5HQSBj9BEFDfgNBjfNagqNhCcEYU29syMjj/77bwHuLtlLsK+HOzqv5Tc4LhObvgCFXwsh/QbPWXodZb1lCMMbUeUtTs3hu7npmr9xOaHAQ1/cv5g/5z9Ek7XvoMAgufQviKj3N3lTCEoIxpk5SVeauzeD5eev5aUMmUREh3HRSLL/3vUvTxc9DWHM45zEYepU1D1UTSwjGmDrD51M2Z+5l4cZMXv5hI6u359KhRQT/GNOby6OWEPH1TZCbDoOvgDPuhmZtvA65QbGEYIzxxP5iH+t25pKcnsPK9ByS07NZtS2XvMJiAHq2a86j4wcytmMeYV/cAhvmQvsBMOE1iB/mbfANlCUEY0yNyy8sZvX2HJLTc0jemsOK9GzW7chjf4kPgKZhwfTpEMWFQzrSLzaKfrEt6Ns6mKDvHoZPn4awpjDmEUj8rTUP1SBLCMaYapWZv5/k9Gxn5+8e+W/clc+B+2i2ahZGv9gorh6RQL/YFvSLjSKhdbNDdxotKYJVM+Gdf0JOGgy63Gkeah7j1So1GpYQjDFVoqqkZxewYmu22+zj/N+WXXCwTMfoJvSNjeK8gbEHd/4dWkT8+u6iJcWw/jtIfh9Wfew817j9MXDxy9Cp9BN7TU2xhGCM11QhLQn250Kn4yG07j2vt8SnbNyVd9hRf3J6Dll7iwAIEuga05xhXVodavLpEEXLZhU8ccxXAlvmw4r3nRpBfoZz5lCvs6HfhdDjTAi2XVRtsq1tjFdKimDlRzD/aUhf7AwLaQIJI6D7GdBjFLTqCrV8r/6CohLW7sg9bMe/elsu+4pKAAgLDqJX+0hG92tPv47OUX/v9pE0DQtgd+LzQdrPThJY+RHkbYfQptDzLDcJjKqTCbGxsAfkGFPb9mXB4tdhwfNOG3mrbnD89RDdGVK+cv52pzhlWyY4yaH7KOhyEoQ1q9ZQfD5lfUYeS1KzWLIli6WpWazbkUuxz9kvRIaH0Cc26uBRf7/YKLq3bX5kD5xXha2L3CTwIeRsheBwZ+ff/0LoObra18scLtAH5FhCMKa2ZG50ksCSqbA/DxJOguP/BD3O+vVtmTM3usnha9g4D4r2QnCY06TUY5STJGJ6H3HtYVdeIUvdHf/S1CyWpWaR657mGRkRwqD4aAbEtTi4849v2ZSgqjxWUhW2LYXkD5y/rC0QFOrE3f9Cp1koPPLI52uqxBKCMXWBKqQugPlTYPWnIEHQ/2KnRtBhYGDzKC502tpTvoJ1X0HGKmd4VBx0H+nsZLueChFRh01WWFxCcnoOS7dksSQ1i6Wpe0jN3AdAcJDQu30kg+KjGdypJYPio+naplnVdv7+67oj2ekYTv4AMjdAUAh0Pc1NAmOgSXTV52+qzBKCMV4qKYZVbv/A1kUQEe2cQz/s9xAVe3Tzzk5zag4pX8KGeVCYgwaFUNg+kfUthvOdDuKzXTGs3JZDUYnz++7QIoLBnaIZFB/NoPiWHNOxBU3Cqul8/p2rDyWBXWtBgqHLydDvAugz1p5FUAdYQjDGCwXZh/oHslOdTuHh18OgS6u1nTx7XxHLUrNYvnkXuSk/0G7n9wwrWUL/oE0A7AlqSWqr4/F1O4MOQ8bQrl2Hals2ALtSDiWBnSsBcTrD+10AfcfZLSXqmEATQkBnGYnIaOBJIBh4UVUfLDU+HHgdGArsBi5R1U0i0hqYARwLvKqqk93yTYF3gW5ACfCxqt4W6MoZU+fs2eQkgcWvH+ofGPNw2f0DR6i4xMfq7blOs88Wp+lnfUY+4HQhdI/pwaC+xxLRqSVhrQvplvMzLTd8Tcv1c2DBLPj5r9Ax0e2cPgNiB1ctpsyNbp/A+7D9F2dYp+Ph7IedJBDZ7qjW03iv0hqCiAQDa4FRQBqwEJikqiv9ylwPDFDVP4rIROACVb1ERJoBg4H+QP9SCeE4Vf1GRMKAr4H/qOpnFcViNQRTp6hC6s9u/8Anbv/ARU6NIHZQlWe7LXvfwTN+lmzZwy9bsykocm7x0LpZmNvu7zT9DIhvQVREaNkz8pXA1sVO01LKV85rFJq2hm6nO8mh28iKrwDOSj2UBNKXOMPijnVOEe07Dlp0rPJ6mtpTnTWEYUCKqm5wZzwNGAes9CszDrjbfT0DmCIioqr5wPci0t1/hqq6F/jGfb1fRBYDcQHEYoz3SoqdC6nmPw1bk5z+gRNvhGHXVql/IHtfER8vS+e7dRksTc1iR04h4Jzv3zc2iknDOjEoPpohnVoS17LJr6/yLU9QMMQf6/yddgfk74b1cw6d2vrLu065DoMOXffQMRHyd0Lyh04SSFvolIkdDKPudZqEojsd8Tqa+iGQhNARSPV7nwaUvpb8YBlVLRaRbKA1sKuymYtINDAWp0mqrPHXAtcCdOpkX0TjoYJsWDwVFjx3qH9gzCNV6h/w+ZT5G3YzPSmVz1dsp7DYR3yrJgzv2vrgmT99OkQSHlKNN3Jr1hoGjHf+fD7YvuzQmUvfPwbfPQJhkU6TFwrtjoGR/3SSQKuu1ReHqbMCSQhlHY6UbmcKpMyvZywSArwNPHWgBvKrmai+ALwATpNRZfM0ptod7B+Y6txeovMIOPu/ztW1R3jnzbQ9e5mxKI13k9LYmrWPqIgQLjk2ngmJ8fSLjQr86P9oBQU5R/2xg+Hkvzn3Dtowz7nFdGQH5zTRNj1qJxZTZwSSENKAeL/3cUB6OWXS3J18CyAzgHm/AKxT1ScCKGtM7TrQP7DqY6d/oN+FzvUDsYOPaDYFRSXMTt7Ou0lp/LDeqTSP6N6Gv5/dmzP7tiMitA7czrlJS+h3vvNnGq1AEsJCoIeIdAG2AhOBS0uVmQlcCcwHLgbmaCW91SJyH07i+N2RBm1MjSkphtUfO/0DaQshogWccIPTP3AEHaiqyoqtOUxPSuWjpVvJKSgmrmUTbhrZk4uGdiSuZdMaXAljqqbShOD2CUwGZuOcdvqyqiaLyL1AkqrOBF4CpopICk7NYOKB6UVkExAFhInI+cCZQA5wJ7AaWOxWk6eo6ovVuXLGBKwgx7mlxE/PQfYWaNnF6R8YOAnCmwc8m8z8/Xy4ZCvTk1JZvT2X8JAgzu7fngmJ8Qzv2vrorgQ2pobZhWmmccvbCd8/4V4/kAudT3TuL9RzdMD9AyU+5dt1GbyblMqXK3dQVKIMiGvB+MR4zhsYS4sm5ZwWakwtqdYL04xpkHLS4dVznBuv9bvAuX6g45CAJ9+0K593F6Xy3qKtbM8poGXTUK4YnsD4xDj6dIiqfAbG1DGWEEzjlJMOr54LeRlw9efOufoB2Lu/mFm/bGd6Uio/b8wkSOCUnjH8a2xfRvZpR1jI0V2VbIyXLCGYxidnG7w2FvJ2wBUfVJoMVJXFW7J4NymVT5ZvI6+wmITWTfnbWb24aEgc7VtE1FLgxtQsSwimccnd7iSD3O1w+XsQP6zcohm5hXywJI3pSWmk7MyjSWgw5wzowITEeI5NaFl71wwYU0ssIZjGI3eHkwxy0p1k0Gn4r4oUlfj4ZvVOpiel8c2anZT4lKGdW/LQRcdwzoBYmofbT8Y0XPbtNo1D3k4nGWRvhctnQOfjDxudsjOX6UlpvL84jV15+2nTPJzfndSF8UPj6d428NNOjanPLCGYhu9gMkiFy2ZA5xMAp2/gy5U7eG7eehZvySIkSDi9d1smJMZzSq+YI3tusDENgCUE07DlZcBr5zmnll72LiScCMCGjDzu+Xgl89Zm0LVNM+4c04fzB3ckJjLc44CN8Y4lBNNw5e+C189zbk532XRIGMHe/cVMmZPCi99tJDwkiH+e25crju9stQFjsIRgGqr83U7NIHMDXDodTTiJz37Zxn2frCQ9u4CLhsTx97N70TbSThk15gBLCKbh2Zvp1Awy18OkaaQ0H8LdL/3M9ym76NshiqcmDSYxwR78bkxplhBMw7I306kZ7FrHvvFv8via9rz8/Xc0DQvm3nH9uOy4zgTbDeaMKZMlBNNwuDUD3bWW+cdN4eb3hR05G7gkMZ5bR/eidXPrMDamIpYQTMOwNxNeH4cvYy0PtriLF+Y045iOETx3+VAGd2rpdXTG1AuWEEz9t28PJa+fj+5YxbX7b2Gx9uL+C3ox8dhO1jxkzBGwhGDqNd23hz3PnUPz7DX8oehmOiSO5Zsze9GyWZjXoRlT71hCMPXWqo1bCHnzIjoXree/Lf7BzRN+y4C4aK/DMqbesoRg6p3svUVM+XwR5yy9nu5Bm/np2Ce5Y8zl9nhKY46SJQRTb/h8yoxFaUz5bDFPFt9L/+DNFJ7/CicNPM/r0IxpECwhmHpheVoW//womZTUdN6LfJSeugmZ8Bohfc71OjRjGgxLCKZO25O/n4e/WMPbP28hvqmPuR2m0Dp7HTLhVbBkYEy1soRg6qQSnzJt4RYenr2G3IJi/nBcW/66605Ctv4C41+BPmO9DtGYBiegWzyKyGgRWSMiKSJyWxnjw0XkHXf8AhFJcIe3FpFvRCRPRKaUmmaoiPziTvOU2PMIjWvJlj2c//QP3PnBCnq2i2TWdYO5LfMuQrYmwcUvQd9xXodoTINUaQ1BRIKBp4FRQBqwUERmqupKv2LXAHtUtbuITAQeAi4BCoC7gP7un79ngWuBn4BZwGjgs6NbHVOf7c4r5KHPVzM9KY22keE8OXEQ5/WJQt6cAKk/w0UvQr8LvA7TmAYrkCajYUCKqm4AEJFpwDjAPyGMA+52X88ApoiIqGo+8L2IdPefoYh0AKJUdb77/nXgfCwhNErFJT7e+nkLj8xew979Jfzh5K78eWQPmkshvDkBUn9ykkH/C70O1ZgGLZCE0BFI9XufBhxXXhlVLRaRbKA1sKuCeaaVmmfHsgqKyLU4NQk6deoUQLimPknalMldHyWzalsOJ3ZvzT3n9aN720jYvxfeugS2/AgX/h/0v8jrUI1p8AJJCGW17WsVylSpvKq+ALwAkJiYWNE8TT2yJ38///50Je8v3kqHFhE8fekQxhzTHhFxk8EE2PwDXPACHHOx1+Ea0ygEkhDSgHi/93FAejll0kQkBGgBZFYyz7hK5mkaqGWpWVz/5mJ25hZw/andmHx6d5qGuV/F/Xvh7Utg0/dwwfMwYLy3wRrTiARyltFCoIeIdBGRMGAiMLNUmZnAle7ri4E5qlru0byqbgNyRWS4e3bRb4CPjjh6U6+oKlN/2sz45+YD8N51J3Dr6N6HkkHRPpg2CTZ+B+c/CwMv8TBaYxqfSmsIbp/AZGA2EAy8rKrJInIvkKSqM4GXgKkikoJTM5h4YHoR2QREAWEicj5wpnuG0nXAq0ATnM5k61BuwPbuL+bOD1bwwZKtnNIzhicuGXT4HUmL9sHbk2DDPDj/GRg0ybtgjWmkpIID+TonMTFRk5KSvA7DHKENGXlc98Zi1u7M5eYzejL5tO6H34iuqACmXQrr58C4p2HwZd4Fa0wDJCKLVDWxsnJ2pbKpUZ+v2MZf311OaLDw2tXDOLlnzOEFigrgnctg/ddw3hRLBsZ4yBKCqRFFJT7++/lq/u+7jQyMj+aZy4bQMbrJ4YWKC+GdyyHlKxj7FAy5wptgjTGAJQRTA3bmFDD5rSX8vCmT3xzfmTvP6UN4SPChAqqwZxN89ndI+RLOfQKGXlnu/IwxtcMSgqlWP23YzeS3lpBfWMwTlwzi/AFtIWMVbF8O25Y7/7f/AoU5zgTnPg6JV3sbtDEGsIRgqomq8vI3yXz29Vdc3jydq/rlEP3zA/DJKigpdAqFNIH2/eGY8dBhAMQNg3Z9vQ3cGHOQJQRTNfm7Yfsy2Lacoq1L2ZWyiKv3p3JNqEIhsLkltB8Ax10L7Qc6CaB1dwgKrnTWxhhvWEIwFVOF7NRDzT0H/ue/6t22AAAYX0lEQVRsPVhkt8SwoqQz6d3GMOS4k5EOAyGqI9gdzY2pVywhmENKimH3Or+2fjcBFGQ54yUIWveAzidA+wF8lxfL37734YtoxdNXDmFoQitv4zfGHBVLCI1V0T7YsfJgsw/blzvvi/c540MioG1f6He+0/TTYaDzPqwpBUUl3PvJSt5asIXhXVvx1KTBtI2M8HZ9jDFHzRJCY5OTDu9cAelLQEucYREtnJ1+4m+dtv72A6BNTwj+9dcjNXMvf3prMcvTsvnjKd3465k9CQkO6MF7xpg6zhJCY/PVPc5pnyfd4hz1tx8A0Z0Cau+fu2YnN72zlJIS5fkrhnJWv/a1ELAxprZYQmhMti6G5dNgxC1w+j8CnqzEpzz59Tr+N2cdvdpF8tzlQ0lo06wGAzXGeMESQmOhCrPvgGYxMOLmgCfLzN/PjdOW8N26XVw0JI77zu9PkzA7ddSYhsgSQmOxaiZsmQ9jn4SIqIAmWZqaxfVvLGJX/n4euPAYJh4b7zzRzBjTIFlCaAyKC+HLf0LbfjC48hvIqSpv/LSZez9ZSbuoCN774wkcE9eiFgI1xnjJEkJjsOB552ZyV3xY6ZXCe/cXc8f7v/Dh0nRO6xXD45cMIrppWIXTGGMaBksIDV3+Lvj2YehxFnQ7rcKi6zPyuO6NRazbmcdfRvXkT6UfZGOMadAsITR0cx+A/flw5n0VFvvsl238bcZywkKCeP23wzipR0yF5Y0xDY8lhIZs52pIegWOvQZiepZZxP9BNoPcB9nEln6QjTGmUbCE0JB9cSeEN4dTby9z9I6cAia/tZiFm/Zw1QkJ3DGmD2EhdtWxMY2VJYSGat1XzqMpz7wfmv76pnP+D7J5cuIgxg3q6EGQxpi6JKDDQREZLSJrRCRFRG4rY3y4iLzjjl8gIgl+4253h68RkbP8ht8sIskiskJE3hYRuztadSkpdmoHrbrCsGsPG6WqPD9vPZe9uICoJiF8NPlESwbGGCCAhCAiwcDTwNlAX2CSiJR+zNU1wB5V7Q48DjzkTtsXmAj0A0YDz4hIsIh0BG4AElW1PxDsljPVYfFrkLEaRt0LIYefMjr1p8088NlqRvdrz8zJI+jZLtKjII0xdU0gNYRhQIqqblDV/cA0YFypMuOA19zXM4CR4lzSOg6YpqqFqroRSHHnB05zVRMRCQGaAulHtyoGgIJs+OY/0HkE9D73sFHJ6dnc98kqTusVw/8mDaZ5uLUYGmMOCSQhdARS/d6nucPKLKOqxUA20Lq8aVV1K/AIsAXYBmSr6hdVWQFTynePwt7dcNb9h93BNL+wmD+/tYSWzUJ5ZPxAu77AGPMrgSSEsvYcGmCZMoeLSEuc2kMXIBZoJiKXl7lwkWtFJElEkjIyMgIItxHL3Ag/PQuDLoXYQYeNuuvDFWzanc8TlwymdfNwjwI0xtRlgSSENCDe730cv27eOVjGbQJqAWRWMO0ZwEZVzVDVIuB94ISyFq6qL6hqoqomxsTYxVIV+upuCAqB0+86bPB7i9J4f8lW/nx6D47v1tqb2IwxdV4gCWEh0ENEuohIGE7n78xSZWYCV7qvLwbmqKq6wye6ZyF1AXoAP+M0FQ0XkaZuX8NIYNXRr04jtnk+rPwQTrwJojocHLw+I4+7PlrBcV1accPIHh4GaIyp6yrtVVTVYhGZDMzGORvoZVVNFpF7gSRVnQm8BEwVkRScmsFEd9pkEZkOrASKgT+pagmwQERmAIvd4UuAF6p/9RoJn8951kFkLJww+eDggqIS/vTmYsJDgnhy4mCCrd/AGFMBcQ7k64fExERNSkryOoy6Z9k78MG1cMHzMPDQ2bv//GgFr8/fzMtXJXJ673YeBmiM8ZKILFLVxMrK2X0K6rv9e+HreyB2MBwz4eDgz1ds4/X5m/ndiC6WDIwxAbET0eu7+VMgZytc9CIEOfk9NXMvt85YzoC4Ftw6urfHARpj6gurIdRnOdvg+8ehz3nQ2TlJq6jEx43TluBT+N+kwXazOmNMwKyGUJ/NuQ98xTDqnoODHvtyLYu3ZPG/SYPp3LqZh8EZY+obO3ysr7Ytg6VvwnF/cG5iB3y7NoNn565n0rB4xg6M9ThAY0x9YwmhPlKF2Xc6t7U+6a8A7Mwt4JbpS+nZrjn/PLefxwEaY+ojSwj10ZpZsOk758E3TaLx+ZRb3llGXmExUy4dQpOwYK8jNMbUQ9aHUN8U74cv/gFtesHQqwF4dt56vk/ZxYMXHmO3szbGVJklhPpm4YuQuQEumwHBISRtyuSxL9cydmAslxwbX/n0xhhTDmsyqk/2ZsK8B6Hb6dD9DLL27ueGt5fQMboJ/7mgPyJ2awpjTNVZDaE+mfsgFObCmfejwK0zlpORV8h7151AZESo19EZY+o5qyHUFxlrneaiIVdCu7689uMmvli5g7+P7s2AuGivozPGNACWEOqLL++C0KZw2p2s2JrNf2atZmTvtlwzoovXkRljGghLCPXB+m9g7edw8l/IC23Jn99eQqtmYTw8fqD1Gxhjqo0lhLrOV+KcZhrdCT3uj9z14Qo2787nyYmDaNUszOvojDENiHUq13VL3oAdK+DiV3hv+W4+WLKVm8/oyXFd7VGYxpjqZTWEuqww17mBXfxxpMSM4q4PVzC8aysmn97d68iMMQ2Q1RDqsu8fh/ydFI5/k8lvL6FJWLA9CtMYU2OshlBXZW2BH6fAMRO4b2lTVm/P5dEJA2kXFeF1ZMaYBsoSQl311T0gwjdxf2TqT5u59uSunNarrddRGWMaMEsIdVHqQlgxg+zBf+SGz3YxMD6av57Zy+uojDENnCWEukYVZt+ONm/HHzeeBAr/m2iPwjTG1LyA9jIiMlpE1ohIiojcVsb4cBF5xx2/QEQS/Mbd7g5fIyJn+Q2PFpEZIrJaRFaJyPHVsUL13or3IG0hn8X8jvlphTx40QA6tW7qdVTGmEag0oQgIsHA08DZQF9gkoj0LVXsGmCPqnYHHgcecqftC0wE+gGjgWfc+QE8CXyuqr2BgcCqo1+deq5oH3x1N7kt+zB5VR8mDevEOQM6eB2VMaaRCKSGMAxIUdUNqrofmAaMK1VmHPCa+3oGMFKceyqMA6apaqGqbgRSgGEiEgWcDLwEoKr7VTXr6FennvvpGchO5W/ZE+jRrgX/Gls67xpjTM0JJCF0BFL93qe5w8oso6rFQDbQuoJpuwIZwCsiskREXhSRZmUtXESuFZEkEUnKyMgIINx6Km8n+t1jJEUMZ25RH6ZcOpiIUHsUpjGm9gSSEMq6CkoDLFPe8BBgCPCsqg4G8oFf9U0AqOoLqpqoqokxMTEBhFtPzbkPX9E+/pY9nnvP608PexSmMaaWBZIQ0gD/ZzPGAenllRGREKAFkFnBtGlAmqoucIfPwEkQjdP2FeiSqbxeNIpjBgxlfGKc1xEZYxqhQBLCQqCHiHQRkTCcTuKZpcrMBK50X18MzFFVdYdPdM9C6gL0AH5W1e1AqogcOLl+JLDyKNelflKl6LM7yNWmvBd1GffbozCNMR6p9F5GqlosIpOB2UAw8LKqJovIvUCSqs7E6RyeKiIpODWDie60ySIyHWdnXwz8SVVL3Fn/GXjTTTIbgKured3qBV07m9DN83io5Dc8cOkp9ihMY4xnxDmQrx8SExM1KSnJ6zCqT0kRWY8msjuvkG/PmMnVJ/f0OiJjTAMkIotUNbGycnb5q4fSv36G6L2bmNX+Oq46qYfX4RhjGjlLCB7Jy9pFsx8fJkmO4fLf/NH6DYwxnrOE4AFVJem124jUPCLOfZCWzcO9DskYYywheOGzeT9wQub7rGp/Hv2HjvA6HGOMASwh1LqUnbmEfXM3vqBQel/6kNfhGGPMQZYQalFBUQkvvPY6Z8hCik+4ieAWduM6Y0zdYQmhFt338S/8JvcFCpp2oPmpN3kdjjHGHKbSC9NM9Zj1yzYKFr1F/9BNMPpFCG3idUjGGHMYSwi1IDVzL/9+7yc+DX8XX/uhBPW/yOuQjDHmVywh1LCiEh83TFvCzfoGLTULGfMwBFlLnTGm7rE9Uw179Iu1hKf9yAT5Chl+PcQN9TokY4wpk9UQatC3azN4dd5Kvo96FZolwGl3eh2SMcaUyxJCDdmZW8At05fy76iZtNmfBpM+hrCmXodljDHlsoRQA3w+5ZZ3lpFQuIaLQz6CIVdCl5O9DssYYypkCaEGPDtvPQtStrMw5lWEdjDqXq9DMsaYSllCqGaLNmfy2JdrebLjXKJ3r4OJb0OTaK/DMsaYStlZRtUoe28RN7y9lBOjMjhnz1TodyH0HuN1WMYYExCrIVQTVeXW95axK3cvX8S+guRFwtn/9TosY4wJmNUQqskbP21mdvIOXu27hGYZS+Dsh6B5jNdhGWNMwCwhVIOV6Tn8+9NVTOhazPBNz0CPM+GY8V6HZYwxR8QSwlHKLyxm8tuLiY4I4b6Q/0MkGM59HOyRmMaYesYSwlH618xkNu7K563EdYRt+Q5G3QMt4rwOyxhjjlhACUFERovIGhFJEZHbyhgfLiLvuOMXiEiC37jb3eFrROSsUtMFi8gSEfnkaFfECx8sSWPGojRuOzGa7ksehM4nwtCrvQ7LGGOqpNKEICLBwNPA2UBfYJKI9C1V7Bpgj6p2Bx4HHnKn7QtMBPoBo4Fn3PkdcCOw6mhXwgsbd+Xzjw9WMKxzS36fMwVKCmHsU3YnU2NMvRXI3msYkKKqG1R1PzANGFeqzDjgNff1DGCkiIg7fJqqFqrqRiDFnR8iEgecA7x49KtRuwqLS5j81mJCQ4J4PjGVoLWz4NTboU13r0MzxpgqCyQhdARS/d6nucPKLKOqxUA20LqSaZ8AbgV8FS1cRK4VkSQRScrIyAgg3Jr34GerSU7P4cmxnWg5907oMAiOn+x1WMYYc1QCSQhlnS6jAZYpc7iInAvsVNVFlS1cVV9Q1URVTYyJ8f68/i9X7uCVHzZx9YkJnLLxcdi3B8ZNgWC7xs8YU78FkhDSgHi/93FAenllRCQEaAFkVjDticB5IrIJpwnqdBF5owrx16r0rH38bcYy+sVGcUePVFg+DUbcDO2P8To0Y4w5aoEkhIVADxHpIiJhOJ3EM0uVmQlc6b6+GJijquoOn+iehdQF6AH8rKq3q2qcqia485ujqpdXw/rUmOISHzdNW0pRsY+nL+5J6Ky/QJtecPLfvA7NGGOqRaXtHKpaLCKTgdlAMPCyqiaLyL1AkqrOBF4CpopICk7NYKI7bbKITAdWAsXAn1S1pIbWpUY9NSeFnzdl8sQlg0hY8l/I2QrXfAEh4V6HZowx1UKcA/n6ITExUZOSkmp9uT+u38VlLy7goiFxPDIsH145G467Ds5+sNZjMcaYIyUii1Q1sbJy1hNaid15hdw0bSld2jTjnrO7wsunQHQnOP0fXodmjDHVyhJCBXw+5S/vLiNrXxGvXj2MZj89Cpnr4YoPILy51+EZY0y1sstqK/DyDxuZuyaDu87pQ182wA9PwaDLodvpXodmjDHVzhJCOZalZvHQ56s5q187Lj82Fj6aDM3awFn3eR2aMcbUCGsyKkNOQRF/fnsJbSMj+O9FA5Efn4Qdv8Alb0CTll6HZ4wxNcISQimqyh3v/8LWrH1M/8NwWuRvhHkPQd9x0Ges1+EZY0yNsSajUt5ZmMony7dxy6ieDI2PhpmTIbQpnP2w16EZY0yNshqCn7U7crn742RGdG/Ddad0g4UvQOoCOP85iGzndXjGGFOjrIbgKihybmndPDyExy4ZSFD2FvjqHug2EgZO9Do8Y4ypcZYQXPd+spK1O/J4bMIg2jYPh09uckaMfcKej2yMaRSsyQj4dPk23lqwhT+e0o2Te8bA0rdg/Ryn3yC6k9fhGWNMrWj0NYTUzL3c9t5yBneK5i9n9oTcHfD57RA/HI79ndfhGWNMrWnUCaGoxMfkt5eAwFMTBxMaHASz/gpF+5yH3tjzkY0xjUij3uM9MnuNc0XyRQOIb9UUVn4Eq2bCqX+HNj28Ds8YY2pVo00Ic9fs5PlvN3DZcZ0Yc0wH51GYn/7VefrZCTd4HZ4xxtS6RtmpvDOngL9MX0bv9pHcdW5fZ+DsO2Hvbrh8BgSHehugMcZ4oNHVEEp8yk3vLGXv/hKmXDqYiNBgSPkalr4JJ94IHQZ6HaIxxnii0dUQnp2bwo/rd/PfiwbQvW0kFObBxzdB6+5wyt+9Ds8YYzzTqBLCwk2ZPP7VOs4bGMv4xDhn4Jx/Q/YWuPpzCI3wNkBjjPFQo2kyytq7nxvfXkJcyybcf0F/RAS2LIAFz8Oxv4fOx3sdojHGeKpR1BBUlVtnLCcjr5D3rjuByIhQKCpw7mTaIg7O+JfXIRpjjOcCqiGIyGgRWSMiKSJyWxnjw0XkHXf8AhFJ8Bt3uzt8jYic5Q6LF5FvRGSViCSLyI3VtUJleX3+Zr5YuYO/j+7NgLhoZ+C3D8OutXDuExAeWZOLN8aYeqHShCAiwcDTwNlAX2CSiPQtVewaYI+qdgceBx5yp+0LTAT6AaOBZ9z5FQN/UdU+wHDgT2XMs1oUlfh4ff4mTu/dlmtGdHEGblsOPzwBAydBjzNqYrHGGFPvBNJkNAxIUdUNACIyDRgHrPQrMw642309A5giIuIOn6aqhcBGEUkBhqnqfGAbgKrmisgqoGOpeVaL0OAg3r/+RHw+dfoNSoqdpqImLeGs/1T34owxpt4KpMmoI5Dq9z7NHVZmGVUtBrKB1oFM6zYvDQYWlLVwEblWRJJEJCkjIyOAcH+tRZNQWjYLc97M/x9sWwZjHoGmrao0P2OMaYgCSQhlPQxAAyxT4bQi0hx4D7hJVXPKWriqvqCqiaqaGBMTE0C4FdiVAt88AL3PdZ6RbIwx5qBAEkIaEO/3Pg5IL6+MiIQALYDMiqYVkVCcZPCmqr5fleCPiM8HM/8MIRFwzqP20BtjjCklkISwEOghIl1EJAynk3hmqTIzgSvd1xcDc1RV3eET3bOQugA9gJ/d/oWXgFWq+lh1rEilFr0MW36Es+6HyPa1skhjjKlPKu1UVtViEZkMzAaCgZdVNVlE7gWSVHUmzs59qttpnImTNHDLTcfpLC4G/qSqJSIyArgC+EVElrqLukNVZ1X3CgKQlQpf/gu6ngqDL6+RRRhjTH0nzoF8/ZCYmKhJSUlHNpEqvDkeNv8A18+Hlgk1EpsxxtRVIrJIVRMrK9fwr1T2lUDbPtDjTEsGxhhTgYafEIJD4Mx/ex2FMcbUeY3m5nbGGGMqZgnBGGMMYAnBGGOMyxKCMcYYwBKCMcYYlyUEY4wxgCUEY4wxLksIxhhjgHp26woRyQA2V3HyNsCuagynvrPtcYhti8PZ9jikoWyLzqpa6fMD6lVCOBoikhTIvTwaC9seh9i2OJxtj0Ma27awJiNjjDGAJQRjjDGuxpQQXvA6gDrGtschti0OZ9vjkEa1LRpNH4IxxpiKNaYagjHGmApYQjDGGAM0goQgIqNFZI2IpIjIbV7H4yURiReRb0RklYgki8iNXsdUF4hIsIgsEZFPvI7FSyISLSIzRGS1+x053uuYvCQiN7u/kxUi8raIRHgdU01r0AlBRIKBp4Gzgb7AJBHp621UnioG/qKqfYDhwJ8a+fY44EZglddB1AFPAp+ram9gII14m4hIR+AGIFFV+wPBwERvo6p5DTohAMOAFFXdoKr7gWnAOI9j8oyqblPVxe7rXJwffEdvo/KWiMQB5wAveh2Ll0QkCjgZeAlAVferapa3UXkuBGgiIiFAUyDd43hqXENPCB2BVL/3aTTyHeABIpIADAYWeBuJ554AbgV8Xgfisa5ABvCK23z2oog08zoor6jqVuARYAuwDchW1S+8jarmNfSEIGUMa/Tn2YpIc+A94CZVzfE6Hq+IyLnATlVd5HUsdUAIMAR4VlUHA/lAo+1zE5GWOK0JXYBYoJmIXO5tVDWvoSeENCDe730cjaDaVxERCcVJBm+q6vtex+OxE4HzRGQTTnPi6SLyhrcheSYNSFPVAzXGGTgJorE6A9ioqhmqWgS8D5zgcUw1rqEnhIVADxHpIiJhOJ1CMz2OyTMiIjhtxKtU9TGv4/Gaqt6uqnGqmoDz3Zijqg3+KLAsqrodSBWRXu6gkcBKD0Py2hZguIg0dX83I2kEnewhXgdQk1S1WEQmA7NxzhJ4WVWTPQ7LSycCVwC/iMhSd9gdqjrLw5hM3fFn4E334GkDcLXH8XhGVReIyAxgMc7ZeUtoBLexsFtXGGOMARp+k5ExxpgAWUIwxhgDWEIwxhjjsoRgjDEGsIRgjDHGZQnBGGMMYAnBGGOM6/8Bh7MCom+zxWUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viewer = cnn.Viewer('./exps/unet/exp01')\n",
    "viewer.graph(name='enet', node='errors', target='dsc-tumor', key=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
