{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "The neural network development and training pipeline consists of a series of high-level templated classes that provide key functionality to define and train a deep learning algorithm. To create a new algorithm using this pipeline, one needs simply to overload the appropriate classes with project specific values. Varying levels of customization are possible by choosing to overload as many class methods and/or variables as needed (e.g. or to leave unchanged with default settings).\n",
    "\n",
    "This tutorial and series of unit tests covers an overview of the neural network development and training pipeline including: \n",
    "\n",
    "1. Overloading `cnn.utils.Client()` to define data loading\n",
    "2. Overloading `cnn.Model()` to define network architecture\n",
    "3. Instantiating a new `cnn.Network()` object and network training\n",
    "4. Loading network statistics\n",
    "5. Running inference\n",
    "\n",
    "The cnn package for implementing all functionality described here can be imported with a single module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overloading cnn.utils.Client()\n",
    "\n",
    "The `cnn.utils.Client()` class provides an object for easy interaction with imported data. While it may be used in isolation, defining several key method overloads will prepare this object with the necessary modifications to be used for CNN training. At minimum, the two key methods that need to be overloaded are:\n",
    "\n",
    "* `self.init_client()`\n",
    "* `self.get()`\n",
    "\n",
    "An example overloaded class definition is provided below. Continue reading for more information about considerations for overloading the `client` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Client(cnn.utils.Client):\n",
    "    \n",
    "    def init_client(self):\n",
    "        \"\"\"\n",
    "        Method to set default experiment specific variables\n",
    "        \n",
    "        \"\"\"\n",
    "        self.infos = {\n",
    "            'shape': [1, 28, 28],\n",
    "            'tiles': [0, 0, 0]\n",
    "        }\n",
    "        \n",
    "        self.inputs = {\n",
    "            'dtypes': {},\n",
    "            'shapes': {},\n",
    "            'classes': {}\n",
    "        }\n",
    "        \n",
    "        self.inputs['dtypes']['dat'] = 'float32'\n",
    "        self.inputs['shapes']['dat'] = [1, 28, 28, 1]\n",
    "        \n",
    "        self.inputs['dtypes']['sce-digit'] = 'int32'\n",
    "        self.inputs['shapes']['sce-digit'] = [1, 1, 1, 1]\n",
    "        self.inputs['classes']['sce-digit'] = 10\n",
    "        \n",
    "    def get(self, mode='mixed'):\n",
    "        \"\"\"\n",
    "        Method to load a single train/valid study\n",
    "        \n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        next_doc = self.next_doc(mode=mode, random=True)\n",
    "        vol = self.load(doc=next_doc['doc'], infos=next_doc['infos'])\n",
    "        \n",
    "        # Preprocessing\n",
    "        vol['dat'] = (vol['dat'] - np.mean(vol['dat'])) / np.std(vol['dat'])\n",
    "        \n",
    "        # Manually add labels from MongoDB document\n",
    "        studyid = next_doc['doc']['study']['studyid']\n",
    "        vol['lbl'] = int(studyid.split('-')[0]) + 1\n",
    "        vol['lbl'] = np.array(vol['lbl']).reshape(1, 1, 1, 1)\n",
    "        \n",
    "        # Create nested vol dictionary\n",
    "        vol['lbl'] = {'sce-digit': vol['lbl']}\n",
    "        \n",
    "        return self.return_get(next_doc, vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading the self.init_client() method\n",
    "\n",
    "The `init_client()` method is called upon initialization of a new `Client()` object to set default experiment-specific values for CNN training. The two main dictionaries to define in this method are `self.infos` and `self.inputs`.\n",
    "\n",
    "### Setting self.infos dictionary\n",
    "\n",
    "The `infos` dictionary defines how underlying data will be loaded from memory. Please see [02_Database_Access_and_Manipulation](02_Database_Access_and_Manipulation.ipynb#Infos-dictionary) for more information.\n",
    "\n",
    "### Setting self.inputs dictionary\n",
    "\n",
    "The `self.inputs` dictionary is used to define key information about inputs into the model, including both input image data and labels. For input images, the corresponding `dtype` and `shapes` (input shape) must be defined. By convention, dictionary key used to define input data is set to `'dat'`.\n",
    "```\n",
    "self.inputs['dtypes']['dat'] = 'float32'\n",
    "self.inputs['shapes']['dat'] = [1, 28, 28, 1]\n",
    "```\n",
    "For input labels, the corresponding `dtypes`, `shapes` and `classes` (total number of classes for classification tasks; set to 0 for regression tasks) must be defined. Note that all labels are assumed to be 4D masks with single value labels represented by a matrix of shape (1, 1, 1, 1). \n",
    "```\n",
    "self.inputs['dtypes']['sce-digit'] = 'int32'\n",
    "self.inputs['shapes']['sce-digit'] = [1, 1, 1, 1]\n",
    "self.inputs['classes']['sce-digit'] = 10\n",
    "```\n",
    "The dictionary key used to define a label must be carefully defined; the library will in fact use the specificiation here to automatically identify logit scores and apply the appropriate loss function without any other user input. To accomplish this, the algorithm assumes that the keys follow a naming convention split into two parts separated by a hypthen (`xxx-xxxx`). The first three letters before the hypthen indicate the type of loss function to apply to this label. The available loss functions include:\n",
    "\n",
    "* `sce`: sigmoid cross-entropy\n",
    "* `dsc`: soft Dice score\n",
    "* `l1d`: L1 distance\n",
    "* `l2d`: L2 distance\n",
    "* `sl1:` smooth L1 loss (Huber)\n",
    "\n",
    "The second half of the key after the hypthen can be any descriptive label as long as the keys are consistent. Note that the keys chosen here must match the keys used in the `self.get()` method below, and potentially in the `cnn.Model()` class below if customizations are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading the self.get() method\n",
    "\n",
    "The `self.get()` method is called during each training iteration to get data prepared for feeding into a CNN. As documented in [02_Database_Access_and_Manipulation](02_Database_Access_and_Manipulation.ipynb#Loading-data), the easiest way to accomplish this is to simply use the `self.next_doc()` method to pick a random MongoDB document matching the prespecified criteria in app_context, and then feeding the document into `self.load()`. Given that the `self.load()` function will only load 2D or 3D mask volume files into the default `vol` dictionary, any 1D conventional classification label if present should be extracted from the corresponding MongoDB document (`next_doc['doc']`) manually. \n",
    "\n",
    "During the `self.get()` method, any number of preprocessing steps may also be included. Note that at this point the tensors remain as Numpy (not Tensorflow) arrays, making a number of preprocessing pipelines easy to implement.\n",
    "\n",
    "There are two data structures that must be returned at the end of this call and passed into the `self.return_get()` method. The first is the `next_doc` dictionary containing the MongoDB document as well as some related metadata. This is generated automatically as part of the call to `self.next_doc()`. The second is a nested dictionary, vol:\n",
    "\n",
    "```\n",
    "vol = {\n",
    "    'dat': (NumPy array)\n",
    "    'msk': msk,\n",
    "    'lbl': lbl\n",
    "}\n",
    "```\n",
    "Note that while the `dat` entry is assumed to contain one input volume, the `lbl` (and `msk`) entries can potentially contain more than one label (and mask). Because of this, while `dat` simply contains a single Numpy array, both `msk` and `lbl` contain dictionaries with a number of potential masks and labels specified by a corresponding key (that matches the same key defined above in `init_inputs()`.\n",
    "\n",
    "Here, `vol['msk']` references a dictionary containing special a mask(s) equal in shape to the label. At all locations where mask is 0, the loss will be masked and not contribute to backpropogation calculations. By default, the mask will be set to 1 (True) for all pixels (or voxels). \n",
    "```\n",
    "msk = {\n",
    "    'lbl-key00': ...,\n",
    "    'lbl-key01': ..., etc\n",
    "}\n",
    "```\n",
    "Here, `vol['lbl']` references a dictionary containing the label. Note that by convention, any label with a value of 0 is ignored (reserved for missing data); thus the first class in your label output should be labeled 1, the second class 2, etc.\n",
    "```\n",
    "lbl = {\n",
    "    'lbl-key00': ...,\n",
    "    'lbl-key01': ..., etc\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overloading cnn.Model()\n",
    "\n",
    "The `cnn.Model()` class provides an object for easy creation and neural network model architectures. Defining several key method overloads will prepare this object with the necessary modifications to be used for CNN training. At minimum, the two key methods that need to be overloaded are:\n",
    "\n",
    "* `self.init_hyperparams_custom()`\n",
    "* custom network definition function \n",
    "\n",
    "An example overloaded class definition is provided below. Continue reading for more information about considerations for overloading the `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(cnn.ClassicModel):\n",
    "    \n",
    "    def init_hyperparams_custom(self):\n",
    "        \n",
    "        self.params['save_dir'] = 'exps/exp01' \n",
    "        self.params['batch_size'] = 128\n",
    "        self.params['iterations'] = 200\n",
    "        \n",
    "        self.params['lnet_fn'] = self.create_lnet\n",
    "        \n",
    "        self.params['train_ratio'] = {'lnet': 1}\n",
    "        self.params['learning_rate'] = {'lnet': 1e-3}\n",
    "        \n",
    "        self.params['stats_top_model_source'] = {\n",
    "            'name': 'lnet',\n",
    "            'node': 'errors',\n",
    "            'target': 'sce-digit',\n",
    "            'key': 'topk'\n",
    "        }\n",
    "    \n",
    "    def create_lnet(self, inputs):\n",
    "        \n",
    "        nn_struct = {}\n",
    "        \n",
    "        nn_struct['channels_out'] = [\n",
    "            16, 16,\n",
    "            32, 32]\n",
    "        \n",
    "        nn_struct['filter_size'] = [\n",
    "            [1, 7, 7], [1, 2, 2],\n",
    "            [1, 7, 7], [1, 2, 2]]\n",
    "        \n",
    "        nn_struct['stride'] = [\n",
    "            1, [1, 2, 2],\n",
    "            1, [1, 2, 2]]\n",
    "        \n",
    "        self.builder.create(\n",
    "            name='L',\n",
    "            nn_struct=nn_struct,\n",
    "            input_layer=inputs[0])\n",
    "        \n",
    "        self.builder.graph.output['layer'] = self.flatten(self.builder.graph.output['layer'])\n",
    "        self.create_logits(name='L')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overloading the self.init_hyperparams_custom() method\n",
    "\n",
    "To define model hyperparameters, overload the `init_hyperparams_custom()` method. Some of the common hyperparameters (with default values) are shown here:\n",
    "\n",
    "```\n",
    "self.params = {\n",
    "    'save_dir': None,               # directory for saving model and metadata\n",
    "    'iterations': 1e6,              # number of training iterations\n",
    "    'batch_size': 1,                # batch size\n",
    "    'learning_rate': None,          # learning rate; no default value (must be set)\n",
    "    'train_ratio': None,            # ratio at which to train different subnetworks; no default value \n",
    "    'optimizer': None,              # optimizer type; by default Adam will be used\n",
    "    'adam_b1': 0.5,                 # b1 for Adam optimizer\n",
    "    'adam_b2': 0.999,               # b2 for Adam optimizer\n",
    "    'l2_beta': 0,                   # lambda constant for L2 regularization\n",
    "}\n",
    "```\n",
    "\n",
    "One key concept to note here is the design choice of *subnetworks*. As needed, defining multiple individual subnetworks using specific conventions (e.g. `lnet`, `enet`, `gnet`, etc) will allow the `cnn` library to orchestrate larger, more complex architectures automatically, and to coordinate the training of each component at specific ratios (specified in the `train_ratio` dictionary entry) and individual learning rates. However the vast majority of standard single-pass feed-forward architectures (classification, U-net, etc) will simply be implemented as just a single *subnetwork*. For standard classification algorithms (VGG, ResNet, Inception, etc) use `lnet` and for fully-convolutional expanding-contracting architectures (U-net, etc) use `enet`. The training ratio for these simple single subnetwork architectures is just `{'lnet': 1}` or `{'enet': 1}` indicating that no special ratio is needed. The corresponding `{'lnet_fn': _}` or `{'enet_fn': _}` simply indicates the particular model architecture function, defined below in the same class, to be used (allows a number of different architecture permutations to be defined in a single template)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture\n",
    "\n",
    "Models are created using the built-in `self.builder` object. To use the `self.builder.create()` three different parameters must be defined. The most important is the `nn_struct` dictionary which defines the structure of the network. This structure is composed predominantly as a series of lists, with each entry in the list corresponding to a single layer in the neural network. For example the first three layers of a CNN may be defined as follows:\n",
    "```\n",
    "nn_struct = {\n",
    "    'channels_out': [16, 32, 64...],\n",
    "    'filter_size': [[1, 3, 3], [1, 3, 3], [1, 3, 3]...],\n",
    "    'stride': [1, 1, 1...]\n",
    " }\n",
    "```\n",
    "\n",
    "In this particular specification, we defined a total of 3 layers, each consisting of 1x3x3 (essentially 2D) convolutional filters with output feature maps 16, 32 and 64 and with a stride of 1. Note that the input channel sizes are calculated automatically. By default, each of these convolutions will be also followed by a batch normalization operation and a ReLU nonlinearity unless otherwise specified. Some of the most common layer specifications are shown here in the order of implementation within a single layer block:\n",
    "```\n",
    "nn_struct = {\n",
    "    'add_preblock': [...],        # name of layer to add before conv (residual connection); default is None\n",
    "    'filter_size': [...],         # filter sizes (specify 3D filters of size [Z, H, W]); no default\n",
    "    'resize': [...],              # perform nearest neighbor resize (specify feature map of size [Z, H, W]); default is None\n",
    "    'batch_norm': [...],          # True to include; default is True\n",
    "    'add_postblock': [...],       # name of layer to add after conv (residual connection); default is None\n",
    "    'relu': [...],                # use 1 for ReLU, values <1 for leaky ReLU; default is 1\n",
    "    'dropout': [...],             # [0, 1] for rate_to_keep; default is NOne\n",
    "}\n",
    "```\n",
    "Note that for each of these options, a value of `None` will ignore this specific layer component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Network\n",
    "\n",
    "To train a network, we use the `cnn.Network` class. While a number of custom modifications may be applied, the default `Network` class will often suffice for common CNN implementations. After initializing a new `Network` class, simply attach your custom class definitions to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define app_context\n",
    "app_context = cnn.db.init_app_context({'name': 'mnist_test'})\n",
    "app_context['tags-series'] = ['mnist-all']\n",
    "\n",
    "# Initialize network\n",
    "net = cnn.Network()\n",
    "net.Client = Client\n",
    "net.Model = Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to initialize (build) the network. In this same step, the library will also inspect your entire database for your requested input data and labels and prepare stratified sampling strategies as needed. The `initialize()` call requires the two required arguments are `app_context` (as defined above) and the fold you wish to set as the validation fold (usually start with 0 and cycle through all the other folds iteratively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(\n",
    "    app_context=app_context,\n",
    "    fold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your graph has been built and compiled by the library, you are ready to train! Doing so requires a simple parameterless call to `train()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default training paradigm implemented by the `cnn` library, training and validation sets are evaluated simultaneously for real-time monitoring of current training dynamics at any given time point. All individual components of the loss function are reported, as are all defined errror metrics (e.g. top-K for classification, Dice score for segmentation, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreaded Training\n",
    "\n",
    "A significant bottleneck to CNN training is loading data into memory (and subsequently GPU) for training. Given the single-threaded nature of Python, typically the data loading process for the next iteration does not begin to occur until the current training iteration has completed. To use a custom asynchronous load function to significantly increase training speed, pass three additional parameters to the `initialize()` call: threads, batch and capacity:\n",
    "\n",
    "* threads: number of separate independent threads to use (consider the # of total CPU threads available on your machine)\n",
    "* batch: total number of exams to be loaded by each thread at a time in a single batch\n",
    "* capacity: total number of studies to be pre-loaded in the queue\n",
    "\n",
    "Note that the number of threads x batch should a multiple of the training batch size (minimum 2 to 3 times greater) otherwise one iteration of the asynchronous processs will not load enough data for a single pass through the network. The following default parameters are reasonable for our starting batch size of 16 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define app_context\n",
    "app_context = cnn.db.init_app_context({'name': 'mnist_test'})\n",
    "app_context['tags-series'] = ['mnist-all']\n",
    "\n",
    "# Initialize network\n",
    "net = cnn.Network()\n",
    "net.Client = Client\n",
    "net.Model = Model\n",
    "net.initialize(\n",
    "    app_context=app_context,\n",
    "    fold=0,\n",
    "    threads=8,\n",
    "    capacity=32)\n",
    "\n",
    "# Train network\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training statistics\n",
    "\n",
    "All training statistics are stored in a Pickled dictionary file. This file can be easily loaded and viewed using the `cnn.Viewer` class. To do so, simply pass the training directory to the initial class instantiation. To view training dynamcis over time, simply pass the subnetwork name (`lnet`, `enet`, etc), node (either `errors` or `losses`), target / label. For losses, no other information is required because the error type is defined by the label itself (e.g. `sce-` for sigmoid cross entropy). For errors, an error key is needed given that a number of possible error metrics may exist for a given label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the current Python kernel is engaged in algorithm training, it may be worthwhile to load a second kernel (new Jupyter notebook, new Python shell, etc) to concurrently graph training loss / error curves over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = cnn.Viewer('./exps/classic/exp01')\n",
    "viewer.graph(name='lnet', node='errors', target='sce-tumor', key='topk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
